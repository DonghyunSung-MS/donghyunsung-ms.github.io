<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"donghyunsung-ms.github.io","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"right","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta property="og:type" content="website">
<meta property="og:title" content="Welcome to Donghyun&#39;s Blog">
<meta property="og:url" content="https://donghyunsung-ms.github.io/index.html">
<meta property="og:site_name" content="Welcome to Donghyun&#39;s Blog">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="Donghyun Sung">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://donghyunsung-ms.github.io/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'en'
  };
</script>

  <title>Welcome to Donghyun's Blog</title>
  
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-171013578-1"></script>
    <script>
      if (CONFIG.hostname === location.hostname) {
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'UA-171013578-1');
      }
    </script>






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Welcome to Donghyun's Blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">Research & Daily Life</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://donghyunsung-ms.github.io/2020/07/27/OptimalControl/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="Donghyun Sung">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Welcome to Donghyun's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/07/27/OptimalControl/" class="post-title-link" itemprop="url">Optimal Control(Part 1 - LQR)</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-07-27 18:18:19" itemprop="dateCreated datePublished" datetime="2020-07-27T18:18:19+09:00">2020-07-27</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-12-27 03:53:06" itemprop="dateModified" datetime="2020-12-27T03:53:06+09:00">2020-12-27</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Reinforcement-Learning/" itemprop="url" rel="index"><span itemprop="name">Reinforcement Learning</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Reinforcement-Learning/Model-Based/" itemprop="url" rel="index"><span itemprop="name">Model-Based</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Reinforcement-Learning/Model-Based/Intro/" itemprop="url" rel="index"><span itemprop="name">Intro</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Reinforcement-Learning/Model-Based/Intro/Optimal-Contol/" itemprop="url" rel="index"><span itemprop="name">Optimal Contol</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>This post is to derive popular optimal control framework called Linear Quadratic Regulator(LQR) and other variants. There are lots of method to derive LQR Algorihm. However, we will follow reinforcement learning perspective.</p>
<h3 id="1-Notation"><a href="#1-Notation" class="headerlink" title="1. Notation"></a>1. Notation</h3><ul>
<li>State at time $t$: $x_t$</li>
<li>Action(Input) at time $t$: $u_t$</li>
<li>Policy: $\pi(u_t \vert x_t)$</li>
<li>Cost to go at time t and state $x_t$: $V_{t}(x_t)$</li>
</ul>
<h3 id="2-Objective"><a href="#2-Objective" class="headerlink" title="2. Objective"></a>2. Objective</h3><p>Optimal control is to find sequential action or policy(stochastic or deterministic) from particular state that minimized objective.</p>
<script type="math/tex; mode=display">J = \min_{u_{0:T-1}}x_{T}^TQ_fx_T+\sum_{\tau=0}^{T-1}x_{\tau}^TQx_{\tau}+u_{\tau}^TRu_{\tau}</script><script type="math/tex; mode=display">s.t\,\,x_{t+1} = Ax_t+Bu_t</script><h3 id="3-Value-iteration"><a href="#3-Value-iteration" class="headerlink" title="3. Value iteration"></a>3. Value iteration</h3><p>Similar to value iteration in reinforcement learning, we can apply bellman backup to update current value function(Cost to go).</p>
<p>Let, $V_{t}(x_t) = \min_{u_{t:T-1}} \Big[x_{T}^TQ_fx_T+\sum_{\tau=t}^{T-1}x_{\tau}^TQx_{\tau}+u_{\tau}^TRu_{\tau}\Big]$</p>
<p>Then, $V_{t+1}(x_{t+1}) = \min_{u_{t+1:T-1}} \Big[x_{T}^TQ_fx_T+\sum_{\tau=t+1}^{T-1}x_{\tau}^TQx_{\tau}+u_{\tau}^TRu_{\tau}\Big]$</p>
<p>we can formulate recursive equation.</p>
<script type="math/tex; mode=display">V_{t}(x_t) =  \min_{u_{t}}\Big[ V_{t+1}(x_{t+1}) + x_{t}^TQx_{t} + u_{t}^TRu_{t}\Big]</script><p>the deterministic policy is now become,</p>
<script type="math/tex; mode=display">u_t^{＊}=\pi^{＊}(x_t) =  arg\min_{u_{t}} \Big[V_{t+1}(Ax_t+Bu_t) + u_{t}^TRu_{t}\Big]</script><h3 id="4-Explicit-Policy"><a href="#4-Explicit-Policy" class="headerlink" title="4. Explicit Policy"></a>4. Explicit Policy</h3><p>We can also assume the value function(cost to go) is quadratic by exploiting the property of cost function($J$).</p>
<p>Let, $V_{t}(x_t) = x_t^TP_tx_t+q_t$, where $P_t$ is matrix and $q_t$ is scalar.</p>
<p>From deterministic policy in section 3, we can substitute cost to go function and get gradient w.r.t $u_t$.<br>$F(x_t,u_t) = (Ax_t+Bu_t)^TP_{t+1}(Ax_t+Bu_t)+q_{t+1} + x_{t}^TQx_{t} + u_{t}^TRu_{t}$</p>
<p>$\nabla_{u_{t}} F(x_t,u_t) = 2Ru_t + 2B^TP_{t+1}(Ax_t+Bu_t) = 0$</p>
<script type="math/tex; mode=display">u_t^{＊}=\pi^{＊}(x_t)=-(B^TP_{t+1}B+R)B^TP_{t+1}Ax_t</script><script type="math/tex; mode=display">K_t = -(B^TP_{t+1}B+R)B^TP_{t+1}A</script><p>We can re-write the recursive equation in value iteration by substituting cost to go function with quadratic function.</p>
<script type="math/tex; mode=display">x_t^TP_{t}x_t+q_t = (Ax_t+Bu_t^{＊})^TP_{t+1}(Ax_t+Bu_t^{＊})+q_{t+1} + x_{t}^T Q x_{t} + {u_t^{＊}}^T R u_t^{＊}</script><script type="math/tex; mode=display">x_t^TP_{t}x_t+q_t = (Ax_t+BK_tx_t)^TP_{t+1}(Ax_t+BK_tx_t)+q_{t+1} + x_{t}^TQx_{t} + (K_tx_t)^TRK_tx_t</script><p>Initial Condition<br>$P_T = Q_f$</p>
<p>$q_T = 0$</p>
<p><strong>For t=T-1:0</strong></p>
<ol>
<li><p>$P_{t}=(A+BK_t)^TP_{t+1}(A+BK_t)+K_t^TRK_t+Q$</p>
<p>where,$\,K_t = -(B^TP_{t+1}B+R)B^TP_{t+1}A$</p>
</li>
<li><p>$q_{t}=q_{t+1}$</p>
</li>
</ol>
<p>Optimal action(input) at time t: $u_t^{＊}=K_tx_t$</p>
<p>Cost to go at time t: $V_{t}(x_t) = x_t^TP_tx_t+q_t$</p>
<h3 id="5-Variants"><a href="#5-Variants" class="headerlink" title="5. Variants"></a>5. Variants</h3><h4 id="Linear-Time-Invariant-System-with-Infinite-horizon-control"><a href="#Linear-Time-Invariant-System-with-Infinite-horizon-control" class="headerlink" title="Linear Time Invariant System with Infinite horizon control."></a>Linear Time Invariant System with Infinite horizon control.</h4><p>Iterating $K_t$ matrix to converge or solve Riccati equation.<br> Then, use $u_{t} = K_{ss}x_t$ to control each step.</p>
<h4 id="Linear-dynamics-with-constant-Affine-system"><a href="#Linear-dynamics-with-constant-Affine-system" class="headerlink" title="Linear dynamics with constant(Affine system)"></a>Linear dynamics with constant(Affine system)</h4><p>$x_{t+1} = Ax_{t}+Bu_{t} + c$</p>
<p>Let’s make augmented state.<br>$\begin{bmatrix}x_{t+1}\\\\1\end{bmatrix} = \begin{bmatrix}A&amp;c\\\\O&amp;1\end{bmatrix}\begin{bmatrix}x_{t}\\\\1\end{bmatrix}+\begin{bmatrix}B\\\\O\end{bmatrix}u_t$</p>
<p>Then, following same derivation above you will get.<br>$u_t = K_tz_t$, where $z_t = \begin{bmatrix}x_{t}\\\\1\end{bmatrix}$</p>
<h4 id="Linear-dynamics-with-noise-stochastic-dynamics"><a href="#Linear-dynamics-with-noise-stochastic-dynamics" class="headerlink" title="Linear dynamics with noise(stochastic dynamics)"></a>Linear dynamics with noise(stochastic dynamics)</h4><p>$x_{t+1} = Ax_{t}+Bu_{t} + w_t$, where $E[w_t]=0$ and $E[w_t^Tw_t]=\Sigma_w$</p>
<p>$P_{t}=(A+BK_t)^TP_{t+1}(A+BK_t)+K_t^TRK_t+Q$, which is same as deterministic case.</p>
<p>$q_{t} = E[w_t^TP_{t+1}w_t]+q_{t+1} = Tr(WP_{t+1}) + q_{t+1}$</p>
<p>Control is also same as deterministic case. $u_t = K_tx_t$</p>
<h4 id="Linear-Time-Variant-system"><a href="#Linear-Time-Variant-system" class="headerlink" title="Linear Time Variant system"></a>Linear Time Variant system</h4><p>Change $A$ and $B$ to correspond time step matrix(ex. $A_t$ and $B_t$)</p>
<h4 id="Penalization-for-change-in-control-inputs"><a href="#Penalization-for-change-in-control-inputs" class="headerlink" title="Penalization for change in control inputs"></a>Penalization for change in control inputs</h4><p>From linear system, $x_{t+1} = Ax_{t}+Bu_{t}$</p>
<p>$x_{t+1} = Ax_{t}+Bu_{t-1}+Bu_{t}-Bu_{t-1}$<br>$u_{t} =u_{t-1} + u_{t}-u_{t-1}$</p>
<script type="math/tex; mode=display">\begin{bmatrix}x_{t+1}\\\\u_t\end{bmatrix} = \begin{bmatrix}A&B\\\\O&I_{u}\end{bmatrix}\begin{bmatrix}x_{t}\\\\u_{t-1}\end{bmatrix} + \begin{bmatrix}B\\\\I_{u}\end{bmatrix}\Delta u</script><h4 id="Trajectory-following-for-non-linear-systems-also-applied-to-non-linear-system-stabilization"><a href="#Trajectory-following-for-non-linear-systems-also-applied-to-non-linear-system-stabilization" class="headerlink" title="Trajectory following for non-linear systems(also applied to non-linear system stabilization)"></a>Trajectory following for non-linear systems(also applied to non-linear system stabilization)</h4><p>Let, non-linear system as $x_{t+1} = f(x_t, u_t)$.</p>
<p>Using partial differential w.r.t ($x_t^{ref}$, $u_t^{ref}$) and 1st-order approximation</p>
<script type="math/tex; mode=display">x_{t+1} \simeq f(x_t^{ref}, u_t^{ref}) + \frac{\partial f}{\partial x}(x_t - x_t^{ref}) \frac{\partial f}{\partial u}(u_t - u_t^{ref})</script><p>Subtract next reference state at both side.</p>
<script type="math/tex; mode=display">x_{t+1} - x_{t+1}^{ref} \simeq f(x_t^{ref}, u_t^{ref}) - x_{t+1}^{ref} + \frac{\partial f}{\partial x}(x_t - x_t^{ref}) \frac{\partial f}{\partial u}(u_t - u_t^{ref})</script><p>We can make approximated linearized affine system with</p>
<p>Transformed state: $z_t = \begin{bmatrix}x_{t} - x_{t}^{ref}\\\\1\end{bmatrix}$</p>
<p>Transformed input: $v_t = u_{t} - u_{t}^{ref}$</p>
<script type="math/tex; mode=display">z_{t+1} = A_tz_t + B_tv_t</script><p>where,</p>
<script type="math/tex; mode=display">c = f(x_t^{ref}, u_t^{ref}) - x_{t+1}^{ref}</script><script type="math/tex; mode=display">,A_t = \begin{bmatrix} \frac{\partial f}{\partial x} & c\\\\ O&1 \end{bmatrix}</script><script type="math/tex; mode=display">B_t = \begin{bmatrix} \frac{\partial f}{\partial u}\\\\O \end{bmatrix}</script><p>Now, we get control policy(feedback law).</p>
<script type="math/tex; mode=display">u_t = K_t\begin{bmatrix}x_{t} - x_{t}^{ref}\\\\1\end{bmatrix} + u_{t}^{ref}</script><h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><ul>
<li><a target="_blank" rel="noopener" href="https://people.eecs.berkeley.edu/~pabbeel/cs287-fa19/">CS287 Advanced Robotics</a> Lecture 5</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://donghyunsung-ms.github.io/2020/07/02/mujoco-py-Note/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="Donghyun Sung">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Welcome to Donghyun's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/07/02/mujoco-py-Note/" class="post-title-link" itemprop="url">Mujoco-py Note(continue)</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-07-02 17:55:04" itemprop="dateCreated datePublished" datetime="2020-07-02T17:55:04+09:00">2020-07-02</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-07-30 18:51:23" itemprop="dateModified" datetime="2020-07-30T18:51:23+09:00">2020-07-30</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Notes/" itemprop="url" rel="index"><span itemprop="name">Notes</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h3 id="Sim-data"><a href="#Sim-data" class="headerlink" title="Sim.data"></a>Sim.data</h3><ul>
<li>qpos<br>If you have free joint in MJCF, qpos[0] to qpos[6] contains xyz position, wxyz quaternion w.r.t world frame.<br>Rest of values are joint angle(revolute) or position(prismatic) in order of xml file(up to bottom).</li>
<li>qvel<br>If you have free joint in MJCF, qpos[0] to qpos[5] contains xyz linear velocity, xyz angular velocity w.r.t world frame.<br>Rest of values are joint angular velocity(revolute) or linear velocity(prismatic) in order of xml file(up to bottom).</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://donghyunsung-ms.github.io/2020/06/29/Trust-Region-Policy-Optimization-Literature-Survey/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="Donghyun Sung">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Welcome to Donghyun's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/06/29/Trust-Region-Policy-Optimization-Literature-Survey/" class="post-title-link" itemprop="url">Trust Region Policy Optimization-Literature Survey</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-06-29 17:36:04" itemprop="dateCreated datePublished" datetime="2020-06-29T17:36:04+09:00">2020-06-29</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-12-27 03:56:16" itemprop="dateModified" datetime="2020-12-27T03:56:16+09:00">2020-12-27</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Reinforcement-Learning/" itemprop="url" rel="index"><span itemprop="name">Reinforcement Learning</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Reinforcement-Learning/Model-Free/" itemprop="url" rel="index"><span itemprop="name">Model-Free</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Reinforcement-Learning/Model-Free/On-policy/" itemprop="url" rel="index"><span itemprop="name">On-policy</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Reinforcement-Learning/Model-Free/On-policy/TRPO/" itemprop="url" rel="index"><span itemprop="name">TRPO</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>Trust Region Policy Optimization[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1502.05477">paper</a>] is on-policy learning with giving constraint on policy. There are a lot of method to address high-dimensional space decision problem in robotics and game domain. The main idea is using function approximation instead of matrix or table. Even though policy gradient methods have been succeeded, it can not improve policy efficiently.<br>The following post illustrates 2002 [<a href="chrome-extension://ohfgljdgelakfkefopgklcohadegdpjf/https://www.cs.cmu.edu/~./jcl/papers/aoarl/Final.pdf">paper</a>] first and then covers TRPO paper.</p>
<h3 id="Policy-Improvement-2002-rightarrow-2015"><a href="#Policy-Improvement-2002-rightarrow-2015" class="headerlink" title="Policy Improvement(2002 $\rightarrow$ 2015)"></a>Policy Improvement(2002 $\rightarrow$ 2015)</h3><p>The concepts of both paper are similar. Both paper focus on policy improvement rather than maximize policy objective itself. This formulation guarantees steady improvement in 2002 paper. Also, they prove the boundness and effectiveness of surrogate objective function with theoretical and experimental result.</p>
<h4 id="Policy-Objective-surrogate-objective"><a href="#Policy-Objective-surrogate-objective" class="headerlink" title="Policy Objective(surrogate objective)"></a>Policy Objective(surrogate objective)</h4><ul>
<li><strong>In time repersentation</strong><script type="math/tex; mode=display">\eta(\pi) = E_{s_0,a_0,s_1,...}[\sum_{t=0}^{\infty}\gamma^{t}r(s_{t})]</script></li>
<li><strong>In state representation</strong></li>
</ul>
<p>$\eta(\tilde\pi) = E_{\tau\sim\tilde\pi}[\sum_{t=0}^{\infty}\gamma^{t}r(s_{t})]$, $\,\,\eta(\pi) = E_{\tau\sim\pi}[\sum_{t=0}^{\infty}\gamma^{t}r(s_{t})]$</p>
<p>$\eta(\tilde\pi) = \eta(\pi)-E_{\tau\sim\pi}[\sum_{t=0}^{\infty}\gamma^{t}r(s_{t})] + E_{\tau\sim\tilde\pi}[\sum_{t=0}^{\infty}\gamma^{t}r(s_{t})]$</p>
<p>$\eta(\tilde\pi) = \eta(\pi)-V_{\pi}(s_0) + E_{\tau\sim\tilde\pi}[\sum_{t=0}^{\infty}\gamma^{t}r(s_{t})]$</p>
<p>$\eta(\tilde\pi) = \eta(\pi) + E_{\tau\sim\tilde\pi}[-V_{\pi}(s_0)+\sum_{t=0}^{\infty}\gamma^{t}r(s_{t})]$</p>
<p>$\eta(\tilde\pi) = \eta(\pi) + E_{\tau\sim\tilde\pi}[-V_{\pi}(s_0)+r(s_0)+\gamma V_{\pi}(s_1)+\gamma\{-V_{\pi}(s_1)+r(s_1)+\gamma V_{\pi}(s_2)\}…]$</p>
<p>$\eta(\tilde\pi) = \eta(\pi) + E_{\tau\sim\tilde\pi}[\sum_{t=0}^{\infty}\gamma^t A_{\pi}(s_t,a_t)]$</p>
<p>$\eta(\tilde\pi) = \eta(\pi) + \sum_{t=0}^{\infty}\gamma^t\sum_sP(s_t=s;\tilde\pi)\sum_a\tilde\pi(a_t|s_t) A_{\pi}(s_t,a_t)$</p>
<p><strong>Result</strong></p>
<script type="math/tex; mode=display">\eta(\tilde\pi) = \eta(\pi)+\sum_{s}\rho_{\tilde\pi}(s)\sum_{a}\tilde\pi(a|s)A_{\pi}(s,a)</script><p>It is hard to formulate equation under $\rho_{\tilde\pi}(s)$. In general policy iteration method, policy evaluation is followed by policy improvenment. After improvement, the agent has not experienced or rolled out with new policy so that new discounted unnormalized visitation frequency has not formed yet. The main idea in 2002 and 2015 literature was, instead of using next policy state disturibution(unnormalized), using previous one.</p>
<script type="math/tex; mode=display">L_{\pi}(\tilde\pi) = \eta(\pi)+\sum_{s}\rho_{\pi}(s)\sum_{a}\tilde\pi(a|s)A_{\pi}(s,a)</script><script type="math/tex; mode=display">\pi'\in arg\max_{\pi'}L_{\pi}(\pi')</script><script type="math/tex; mode=display">\pi_{new}=(1-\alpha){\pi_{old}}+\alpha{\pi'}</script><p>They suggest that it can not gaurantee direct maximizing the advantage function is equal to improvement in policy. This is because the advantage in practice is parameterized, which causes estimation error and approximation error at the same time. Instead, they use conservative policy iteration update so that they could provide explicit lower bounds on the improvement of η.</p>
<h5 id="Boundness"><a href="#Boundness" class="headerlink" title="Boundness"></a><strong>Boundness</strong></h5><p>Let’s go with 2002 approach before we dive into 2015 approach which little bit changes in policy improvement.</p>
<ul>
<li><strong>Properties &amp; Condition</strong></li>
</ul>
<p>$\eta(\tilde\pi) = \eta(\pi) + E_{\tau\sim\tilde\pi}[\sum_{t=0}^{\infty}\gamma^t A_{\pi}(s_t,a_t)]$</p>
<p>If $\tilde\pi=\pi$</p>
<p>$E_{\tau\sim\pi}[\sum_{t=0}^{\infty}\gamma^t A_{\pi}(s_t,a_t)]=0$</p>
<p>$\sum_{s}\rho_{\pi}(s)\sum_{a}\pi(a|s)A_{\pi}(s,a)=0$</p>
<p>$\sum_{a}\pi(a|s)A_{\pi}(s,a)=0$  </p>
<p>$\epsilon_{old} = \max_s|E_{a\sim\pi’}A_{\pi}(s,a)|\geq |E_{a\sim\pi’}A_{\pi}(s,a)|$  </p>
<ul>
<li><strong>Derivation</strong></li>
</ul>
<p>$\eta(\pi_{new}) = \eta(\pi_{old}) + E_{\tau\sim\pi_{new}}[\sum_{t=0}^{\infty}\gamma^t A_{\pi}(s_t,a_t)]$<br>$\eta(\pi_{new}) = \eta(\pi_{old}) + \sum_{s}\rho_{\pi_{new}}(s)\sum_{a}\pi_{new}(a|s)A_{\pi_{old}}(s,a)$<br>$\eta(\pi_{new}) = \eta(\pi_{old}) + \sum_{s}\rho_{\pi_{new}}(s)\sum_{a}\{(1-\alpha)\pi_{old}(a|s)+\alpha\pi’(a|s)\}A_{\pi_{old}}(s,a)$<br>$\eta(\pi_{new}) = \eta(\pi_{old}) + \sum_{s}\rho_{\pi_{new}}(s)\sum_{a}\{\alpha\pi’(a|s)\}A_{\pi_{old}}(s,a)$<br>$\eta(\pi_{new}) = \eta(\pi_{old}) + \sum_{t=0}^{\infty}\gamma^t\sum_sP(s_t=s;\pi_{new})\sum_{a}\{\alpha\pi’(a|s)\}A_{\pi_{old}}(s,a)$<br>$\eta(\pi_{new}) = \eta(\pi_{old}) + \sum_{t=0}^{\infty}\gamma^t\sum_s\{(1-\alpha)^t P(s_t=s;\pi_{old\,only})+(1-(1-\alpha)^t) P(s_t=s;\pi_{rest})\}\sum_{a}\{\alpha\pi’(a|s)\}A_{\pi_{old}}(s,a)$<br>Let $r_a$ denotes $1-(1-\alpha)^t$,</p>
<p>$\eta(\pi_{new}) = \eta(\pi_{old}) + \sum_{t=0}^{\infty}\gamma^t\sum_s\{(1-r_a) P(s_t=s;\pi_{old\,only})+r_a P(s_t=s;\pi_{rest})\}\sum_{a}\{\alpha\pi’(a|s)\}A_{\pi_{old}}(s,a)$<br>$\eta(\pi_{new}) = L_{\pi_{old}}(\pi_{new}) + \sum_{t=0}^{\infty}\gamma^t\sum_s\{-r_a P(s_t=s;\pi_{old\,only})+r_a P(s_t=s;\pi_{rest})\}\sum_{a}\{\alpha\pi’(a|s)\}A_{\pi_{old}}(s,a)$<br>$\eta(\pi_{new}) = L_{\pi_{old}}(\pi_{new}) - \sum_{t=0}^{\infty}\gamma^t2r_{a}\epsilon_{old}$</p>
<script type="math/tex; mode=display">\eta(\pi_{new}) \geq L_{\pi_{old}}(\pi_{new}) -\frac{2\alpha^2\epsilon_{old}\gamma}{(1-\gamma)^2}</script><p>This inequality condition means that if we maximize $L_{\pi_{old}}(\pi_{new})$, it gaurantees policy improvements with error term.</p>
<p>Let’s go with 2015 approach. They changes $\epsilon$ definition and give more generality while having more error term. We denotes $\epsilon$ as $\epsilon_{new}$</p>
<p>$\epsilon_{old} = \max_{s}\vert E_{a\sim\pi’} A_{\pi}(s,a)\vert = \max_s\vert\sum_a\pi(a|s)A_{\pi}(s,a)-\sum_a\pi’(a|s)A_{\pi}(s,a)\vert<br>\leq 2*\max_{s,a}\vert A_{\pi}(s,a) \vert =2\epsilon_{new}$</p>
<script type="math/tex; mode=display">\eta(\pi_{new}) \geq L_{\pi_{old}}(\pi_{new}) -\frac{4\alpha^2\epsilon_{new}\gamma}{(1-\gamma)^2}</script><script type="math/tex; mode=display">\eta(\pi_{new}) \geq L_{\pi_{old}}(\pi_{new}) -C * D_{KL}^{max}(\pi_{new}||\pi_{old})</script><script type="math/tex; mode=display">where\,\, C\,=\, \frac{4\epsilon_{new}\gamma}{(1-\gamma)^2},\,\,\alpha\,=\,D_{TV}^{max}(\pi_{new}||\pi_{old})</script><h5 id="Toward-Theory-to-Practical-Implementation-form"><a href="#Toward-Theory-to-Practical-Implementation-form" class="headerlink" title="Toward Theory to Practical Implementation form"></a>Toward Theory to Practical Implementation form</h5><ul>
<li><strong>Change Boundness to Constant</strong><br>In practice, policy($\pi$) is parameteriezed by $\theta$. If we follow theoretical step($C$), it would be small. The literature recommands to choose $\delta$, which changes the optimization problem.<script type="math/tex; mode=display">maximize_{\theta}\:L_{\theta_{old}}(\theta)</script><script type="math/tex; mode=display">subject\:to\:D_{KL}^{max}(\theta||\theta_{old})\leq\delta</script></li>
<li><strong>Exact method to Heuristic approximation</strong><br>It is impractical to calculate $D_{KL}^{max}$ at each iteration. Instead, they choose heuristic approximation($D_{KL}^{\rho}=E[D_{KL}]$)<script type="math/tex; mode=display">maximize_{\theta}\:L_{\theta_{old}}(\theta)</script><script type="math/tex; mode=display">subject\:to\:D_{KL}^{\rho}(\theta||\theta_{old})\leq\delta</script></li>
<li><strong>Expectation becomes Sampled Sum</strong><br>In this section, sampled sum which we call Monte-Carlo simulation replaces expecation  </li>
</ul>
<p>$L_{\pi}(\pi_{new}) = \eta(\pi_{old})+\sum_{s}\rho_{\pi_{old}}(s)\sum_{a}\pi_{new}(a|s)A_{\pi_{old}}(s,a)$</p>
<p>$\sum_{s}\rho_{\pi}(s)\:\rightarrow\:\frac{1}{1-\gamma}E_{s\sim\rho_{old}}[*]$<br>$A_{\pi_{old}}(s,a)\:\rightarrow\:Q_{\pi_{old}}(s,a)$</p>
<p>Importance of sampling($q$ sampling distribution)</p>
<p>$\sum_{a}\pi_{new}(a|s)A_{\pi_{old}}(s,a)\:\rightarrow\:E_{a\sim q}[\frac{\pi_{new}}{q}A_{\pi_{old}}(s,a)]$</p>
<script type="math/tex; mode=display">maximize_{\theta}\:E_{a\sim q,\,s\sim \rho_{old}}g[\frac{\pi_{new}}{q}Q_{\pi_{old}}(s,a)g]</script><script type="math/tex; mode=display">subject\:to\:D_{KL}^{\rho}(\theta||\theta_{old})\leq\delta</script><ul>
<li><strong>Linear Approximation of Objective Function, Quadratic Approximation of Constraint</strong>  </li>
</ul>
<script type="math/tex; mode=display">maximize_{\theta}\:\nabla L \Delta\theta</script><script type="math/tex; mode=display">subject\:to\:\frac{1}{2}\Delta\theta^T A \Delta\theta\leq\delta</script><p>where, $\:A\,$ is Fisherman Information Matrix(FIM), also Hessian of $D_{KL}$</p>
<h3 id="Details-Optional"><a href="#Details-Optional" class="headerlink" title="Details(Optional)"></a>Details(Optional)</h3><h4 id="Lagragian-Method"><a href="#Lagragian-Method" class="headerlink" title="Lagragian Method"></a>Lagragian Method</h4><script type="math/tex; mode=display">F(\Delta\theta,\,\lambda)\,=\,\nabla L^T\,\Delta\theta\,-\,\lambda(\frac{1}{2}\Delta\theta^TA \Delta\theta\,-\,\delta)</script><script type="math/tex; mode=display">\nabla F(\Delta\theta,\,\lambda)=0</script><p>This yields to two equation</p>
<script type="math/tex; mode=display">\nabla L - \lambda A \Delta\theta=0  ---- (1)</script><script type="math/tex; mode=display">\Delta\theta=\frac 1 \lambda A^{-1}\nabla L</script><script type="math/tex; mode=display">\lambda(\frac{1}{2}\Delta\theta^T A \Delta\theta-\delta)=0  ---- (2)</script><p>By KKT condition, $\lambda$ should be positive and $\delta$ should be $\frac{1}{2}\Delta\theta^T A \Delta\theta$.<br>Use second condition and equation 1</p>
<p>$\delta=\frac{1}{2}\Delta\theta^T A \Delta\theta=\frac{1}{2\lambda^2}\nabla L^T A^{-1}\nabla L$</p>
<p>$\frac{1}{\lambda} = \sqrt{\frac{2\delta}{\nabla L^T A^{-1}\nabla L}}$</p>
<p>Finally, we get update law and maximum value</p>
<p>$\Delta\theta = \theta-\theta_{old}= {\sqrt{\frac{2\delta}{\nabla L^T A^{-1}\nabla L}}}A^{-1}\nabla L $</p>
<p>$A^{-1}\nabla L $ denotes <strong>search direction</strong>.</p>
<p>${\sqrt{\frac{2\delta}{\nabla L^T A^{-1}\nabla L}}}$ indicates <strong>step size</strong>.</p>
<p>$ \nabla L^T \Delta\theta = {\sqrt{\frac{2\delta}{\nabla L^T A^{-1}\nabla L}}}{\nabla L^T A^{-1}\nabla L}$</p>
<h4 id="Conjugate-Gradient-Method"><a href="#Conjugate-Gradient-Method" class="headerlink" title="Conjugate Gradient Method"></a>Conjugate Gradient Method</h4><p>This is iterative method to solve $Ax=b$ or $minimize_{x}\: \Phi(x)=\frac{1}{2}x^TAx-b^Tx$.<br>where, $A$ is positive definite.<br>In previous section, we need $A^{-1}\nabla L$ to calculate update law. However, it is comuputationally high to get inverse of hessian($A^{-1}$). Instead, we compute hessian vector product iteratively.<br>In this case $Ax=g$, where $g$ is gradient of objective function($\nabla L$)</p>
<h5 id="Derivation-One-Minute-Derivation-of-The-Conjugate-Gradient-Algorithm"><a href="#Derivation-One-Minute-Derivation-of-The-Conjugate-Gradient-Algorithm" class="headerlink" title="Derivation(One-Minute Derivation of The Conjugate Gradient Algorithm)"></a>Derivation(<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1608.08691">One-Minute Derivation of The Conjugate Gradient Algorithm</a>)</h5><p>we want to solve for $Ax^＊=g$</p>
<p>$x_{i+1}=x_{i}+\alpha_i d_i$</p>
<p>$x_{i+1}-x^＊= x_{i}-x^＊+\alpha_i d_i$</p>
<p>Let, $e_i=x_i-x^＊$</p>
<p>$e_{i+1} = e_{i}+\alpha_i d_i$</p>
<p>$Ae_{i+1} = Ae_{i}+\alpha_i Ad_i$</p>
<p>Let, $r_i=g-Ax_i$</p>
<p>$r_{i+1} = r_{i}-\alpha_i Ad_i$</p>
<p>Let, we choose next residual perpendicular to previous one $r_{i+1}^Tr_i=r_{i}^Tr_{i+1}=0$.</p>
<p>$r_{i}^Tr_{i+1} = r_{i}^Tr_{i}-\alpha_i\, r_{i}^TAd_i=0$</p>
<script type="math/tex; mode=display">\alpha_i = \frac{r_{i}^Tr_{i}}{r_{i}^TAd_i}</script><p>We have to calculate $d_i$ iteratively.</p>
<p>In fact, another approach to explain this part is A-conjugate or A-orthogonal. Vector $d_i$ is n-dimensional, which is basis of A. You can find Gram Schmidt Orthogonalization if you need more information. Briefly, we can build n orthogonal basis vectors from any vector $d_0$ if A is invertible(non-zero eigen values).</p>
<script type="math/tex; mode=display">d_{i}^TAd_{i+1}=0</script><p>Let</p>
<p>$d_{i+1} = r_{i+1} + \beta_{i+1}d_{i}$</p>
<p>$d_{i}^TAd_{i+1} = d_{i}^TAr_{i+1} + \beta_{i+1}d_{i}^TAd_{i}$</p>
<p>$0 = d_{i}^TAr_{i+1} + \beta_{i+1}d_{i}^TAd_{i}$</p>
<script type="math/tex; mode=display">\beta_{i+1} = -\frac{d_{i}^TAr_{i+1}} {d_{i}^TAd_{i}}</script><p>$r_{i+1}^T = r_{i}^T-\alpha_id_i^T A\,\,\,$</p>
<p>$r_{i+1}^Tr_{i+1} = r_{i}^Tr_{i+1}-\alpha_id_i^T Ar_{i+1}\,\,\,$</p>
<p>$d_i^T Ar_{i+1}=-\frac{1}{\alpha_i}r_{i+1}^Tr_{i+1}\,\,\,$</p>
<p>$d_{i} = r_{i} + \beta_{i}d_{i-1}$</p>
<p>$Ad_{i} = Ar_{i} + \beta_{i}Ad_{i-1}$</p>
<p>$d_{i}^TAd_{i} = d_{i}^TAr_{i} + \beta_{i}d_{i}^TAd_{i-1}$</p>
<p>$d_{i}^TAd_{i} = d_{i}^TAr_{i}$</p>
<script type="math/tex; mode=display">\beta_{i+1} = -\frac{d_{i}^TAr_{i+1}}{d_{i}^TAd_{i}} = \frac{1}{\alpha_i}\frac{r_{i+1}^Tr_{i+1}}{d_{i}^TAd_{i}}= \frac{r_{i+1}^Tr_{i+1}}{r_{i}^Tr_{i}}</script><h5 id="Algorithm"><a href="#Algorithm" class="headerlink" title="Algorithm"></a>Algorithm</h5><ol>
<li>Initial guess $x_0$  </li>
<li>Set initial condition $d_0=r_0=b-Ax_0$  </li>
<li>Repeat until $r_0$ is sufficiently small(gradient of objective is near zero or linear system is near solution)<ul>
<li>compute $\alpha$<script type="math/tex; mode=display">\alpha_i = \frac{r_{i}^Tr_{i}}{r_{i}^TAd_i}\,or\,\frac{r_{i}^Tr_{i}}{d_{i}^TAd_i}</script></li>
<li>update $x$<script type="math/tex; mode=display">x_{i+1}=x_{i}+\alpha_i d_i</script></li>
<li>update $r$<script type="math/tex; mode=display">r_{i+1} = r_{i}-\alpha_i Ad_i</script></li>
<li>compute $\beta$<script type="math/tex; mode=display">\beta_{i+1} = \frac{r_{i+1}^Tr_{i+1}} {r_{i}^Tr_{i}}</script></li>
<li>update $d$<script type="math/tex; mode=display">d_{i+1} = r_{i+1} + \beta_{i+1}d_{i}</script></li>
</ul>
</li>
<li>Return $x_n$  </li>
</ol>
<h5 id="How-to-compute-Ad-i"><a href="#How-to-compute-Ad-i" class="headerlink" title="How to compute $Ad_i$"></a>How to compute $Ad_i$</h5><p>We use conjugate gradient method to avoid caculating inverse of hessian matrix. However, there is one problem left. We need A matrix in algorithm, which is KL divergence of hessian. It is well-know that caculating gradient is computationally cheap than getting hessian itself. Therfore, we use under equation.  </p>
<script type="math/tex; mode=display">\nabla^2 KL\: d_i = \nabla(\nabla KL\: d_i)</script><p><center ><br>where $\nabla^2 KL$ is $A$</p>
<p><strong>To be specific</strong></p>
<ol>
<li>Get $KL(\pi,\pi_{old})$</li>
<li>Use auto grad package to get $\nabla KL(\pi,\pi_{old})$</li>
<li>Get $\nabla KL(\pi,\pi_{old})d_{i}$</li>
<li>Again, use auto grad package to get  $\nabla(\nabla KL(\pi,\pi_{old})d_{i})$  </li>
</ol>
<p>Now, you get $Ad_i$ indirectly</p>
<h5 id="Summarry"><a href="#Summarry" class="headerlink" title="Summarry"></a>Summarry</h5><ol>
<li>Compute search direction($A^{-1}\,\nabla L$) using conjugate gradient method(n-step)<ul>
<li>Input : initial guess of $x_0$ ex. zero vector and gradient of objective function</li>
<li>In algorithm, use hessian vector prouct to get $Ad_i$ indirecty</li>
</ul>
</li>
<li>Get step size Using equation</li>
<li>Update parmeter Vector($\theta$)</li>
</ol>
<script type="math/tex; mode=display">\theta= \theta_{old}+{\sqrt{\frac{2\delta}{\nabla L^T A^{-1}\nabla L}}}A^{-1}\nabla L</script>
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://donghyunsung-ms.github.io/2020/06/24/Adavantage-Weighted-Regression-Literature-Survey/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="Donghyun Sung">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Welcome to Donghyun's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/06/24/Adavantage-Weighted-Regression-Literature-Survey/" class="post-title-link" itemprop="url">Adavantage Weighted Regression-Literature Survey</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-06-24 16:32:57" itemprop="dateCreated datePublished" datetime="2020-06-24T16:32:57+09:00">2020-06-24</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-07-30 18:50:13" itemprop="dateModified" datetime="2020-07-30T18:50:13+09:00">2020-07-30</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Reinforcement-Learning/" itemprop="url" rel="index"><span itemprop="name">Reinforcement Learning</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Reinforcement-Learning/Model-Free/" itemprop="url" rel="index"><span itemprop="name">Model-Free</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Reinforcement-Learning/Model-Free/Off-policy/" itemprop="url" rel="index"><span itemprop="name">Off-policy</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Reinforcement-Learning/Model-Free/Off-policy/AWR/" itemprop="url" rel="index"><span itemprop="name">AWR</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>This post is to summary recent RL algorithm called <strong>Adavantage Weighted Regression(AWR)</strong> <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1910.00177">(paper)</a>. Detail derivation and explanation are added to help understand deeply.<br>※Explanation may not be accurate. Readers should read this post carefully.</p>
<h3 id="Contribution-personal"><a href="#Contribution-personal" class="headerlink" title="Contribution(personal)"></a>Contribution(personal)</h3><ul>
<li>This algorithm improves Reward Weighted Regression by using policy improvement instead of direct policy maximization.</li>
<li>It is off-policy algorithm that has higher sample efficiency than on-policy one.</li>
<li>It can learn policy from static expert data set without collecting or sampling data from environment like behavior cloning.</li>
</ul>
<h3 id="Preliminaries"><a href="#Preliminaries" class="headerlink" title="Preliminaries"></a>Preliminaries</h3><p>As always, we want to find policy that maximize return(sum of discounted rewards). We can represent objective as time or state and action.</p>
<script type="math/tex; mode=display">J(\pi)=E_{\tau\sim p_{\pi}(\tau)}\Big[\sum_{t=0}^{\infty}\gamma^t r_t\Big]</script><script type="math/tex; mode=display">J(\pi)=\sum_{t=0}^{\infty}\gamma^t\int_{s}p(s_t = s\vert\pi)\int_{a}\pi(a\vert s)r(s,a)dads</script><script type="math/tex; mode=display">J(\pi)=\int_{s}\sum_{t=0}^{\infty}\gamma^tp(s_t = s\vert\pi)\int_{a}\pi(a\vert s)r(s,a)dads</script><script type="math/tex; mode=display">J(\pi)=\int_{s}d^{\pi}(s)\int_{a}\pi(a\vert s)r(s,a)dads</script><script type="math/tex; mode=display">J(\pi)=E_{s\sim d^{\pi}(s)}\Big[E_{a\sim\pi(a\vert s)}\Big[r(s,a)\Big]\Big]</script><h3 id="AWR-Objective-amp-Derivation"><a href="#AWR-Objective-amp-Derivation" class="headerlink" title="AWR Objective &amp; Derivation"></a>AWR Objective &amp; Derivation</h3><p>As mentioned in contribution section, AWR algorithm maximizes policy improvement. In this equation, it is impossible to get expectation under discounted state distribution following policy($\pi$). According to 2002(Sham Kakade and John Langford) and 2015(TRPO) paper, expectation under sampling policy is tractable and it is approximate of true policy improvement with small error term(boundness).</p>
<script type="math/tex; mode=display">\eta(\pi)=J(\pi)-J(\mu)</script><script type="math/tex; mode=display">\eta(\pi)=E_{\tau\sim p_{\pi}(\tau)}\Big[\sum_{t=0}^{\infty}\gamma^t r_t\Big]-E_{s_0}\Big[V^{\mu}(s_0)\Big]</script><script type="math/tex; mode=display">\eta(\pi)=E_{\tau\sim p_{\pi}(\tau)}\Big[\sum_{t=0}^{\infty}\gamma^t r_t-V^{\mu}(s_0)\Big]</script><script type="math/tex; mode=display">\eta(\pi)=E_{\tau\sim p_{\pi}(\tau)}\Big[\sum_{t=0}^{\infty}\gamma^t (r_t+V^{\mu}(s_{t+1})-V^{\mu}(s_t)\Big]</script><script type="math/tex; mode=display">\eta(\pi)=E_{\tau\sim p_{\pi}(\tau)}\Big[\sum_{t=0}^{\infty}\gamma^t A^\mu(s_t,a_t)\Big]</script><p>As same derivation in preliminaries section, we can get equation under state, action expectation.</p>
<script type="math/tex; mode=display">\eta(\pi)=E_{s\sim d^{\pi}(s)}\Big[E_{a\sim\pi(a\vert s)}\Big[A^\mu(s,a)\Big]\Big]</script><p>“The objective can be difficult to optimize due to the dependency between $d^{\pi}(s)$ and $\pi$, as well as the need to collect samples from $\pi$” - In the paper</p>
<script type="math/tex; mode=display">\hat{\eta}(\pi)=E_{s\sim d^{\mu}(s)}\Big[E_{a\sim\pi(a\vert s)}\Big[A^\mu(s,a)\Big]\Big]</script><p>we can consider this optimization problem as constrained policy search. This is because, according to early paper(2015 TRPO), $\hat{\eta}(\pi)$ is guarantee only when $\pi$ ans $\mu$ are closed enough(the closeness in probability is defined as KL-divergence).</p>
<script type="math/tex; mode=display">arg\max_{\pi}=\hat{\eta}(\pi)</script><script type="math/tex; mode=display">s.t\quad\int_s d^{\mu}(s)D_{KL}(\pi(\cdot|s)\vert\vert\mu(\cdot|s))ds\leq\epsilon</script><script type="math/tex; mode=display">\int_{a}\pi(a\vert s)da = 1</script><p>we can re-write this equation in soft constrained form using Langrangian.</p>
<script type="math/tex; mode=display">L(\pi,\beta,\alpha_s)=\hat{\eta}(\pi)+\beta (\epsilon-\int_s d^{\mu}(s)D_{KL}(\pi(\cdot|s)\vert\vert\mu(\cdot|s))ds)+\int_{a}\alpha_s(1-\pi(a\vert s))da</script><script type="math/tex; mode=display">\frac{\partial L(\pi,\beta,\alpha_s)}{\partial\pi(a\vert s)} = 0</script><script type="math/tex; mode=display">\pi^{*}=\frac{1}{Z(s)}\mu(a\vert s) \exp(\frac{1}{\beta}A^{\mu}(s,a))</script><p>where $Z(s)$ is normalized constant to make $\pi^{*}$ sum up to 1</p>
<p>we want $\pi$ close enough to $\pi^*$ in terms of KL-divergence. we can write this in to optimization problem.</p>
<script type="math/tex; mode=display">arg\min_{\pi}\int_{s}d^\mu (s)D_{KL}(\pi^*(\cdot|s)\vert\vert\pi(\cdot|s))ds</script><p>Following the definition of KL-divergence you can easily get under problem.</p>
<script type="math/tex; mode=display">arg\max_{\pi}\int_{s}d^\mu (s)\int_{a}\mu(a\vert s) \log\pi(a\vert s)\exp(\frac{1}{\beta}A^{\mu}(s,a))ds</script><h3 id="Off-policy-Learning-with-Experience-Replay"><a href="#Off-policy-Learning-with-Experience-Replay" class="headerlink" title="Off-policy Learning with Experience Replay"></a>Off-policy Learning with Experience Replay</h3><p>On-policy learning uses behavior policy(sampling policy) only at k-th iteration trajectory($\tau$) data, which is inefficient. This is because we throw away hole data that collected from previous iterations. Instead, AWR uses hole data in replay buffer($D$). However, state distribution and policy at each iteration are different. This makes expectation of current policy impossible. In AWR derivation, this algorithm considers samples from replay buffer as prior policy that mixture of 1~k iterations.</p>
<h3 id="Algorithm"><a href="#Algorithm" class="headerlink" title="Algorithm"></a>Algorithm</h3><img src="/2020/06/24/Adavantage-Weighted-Regression-Literature-Survey/algo.png" class="">
<ol>
<li>Start from random policy(ex.generated by initial weights in policy network).</li>
<li>At each iteration sample trajectory following current policy($\pi_k$).</li>
<li>Uniformly select $N$ samples from replay buffer($D$).</li>
<li>Update state value function($TD(\lambda)$ as target semi-gradient descent).</li>
<li>Update policy following upper objective.</li>
</ol>
<h3 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h3><p>In the <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1910.00177">paper</a>, there are sections that I do not cover in this post. For specific, the author(Jason Peng) is famous for “deepmimic” which agent learns agile skills from mocap data using RL. He suggests that AWR is better performance than Proximal Policy optimization and Reward Weighted Regression algorithm in terms of fast convergence when do motion imitation tasks. Also, AWR learns from fully static data that collects form expert. He also compares AWR with others that can learn or cloning expert policy in fully off-line manner.</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://donghyunsung-ms.github.io/2020/06/23/RL-Core/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="Donghyun Sung">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Welcome to Donghyun's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/06/23/RL-Core/" class="post-title-link" itemprop="url">RL Core</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-06-23 14:06:49" itemprop="dateCreated datePublished" datetime="2020-06-23T14:06:49+09:00">2020-06-23</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-07-30 18:50:33" itemprop="dateModified" datetime="2020-07-30T18:50:33+09:00">2020-07-30</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Reinforcement-Learning/" itemprop="url" rel="index"><span itemprop="name">Reinforcement Learning</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Reinforcement-Learning/Intro/" itemprop="url" rel="index"><span itemprop="name">Intro</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>This post is to introduce the key concept of reinforcement learning with math. When you want to read and understand recent RL papers, it is sometimes hard to follow the equation under expectation. I hope that this article paves the way to deeper insight towards RL. If you are interested in this topic, it is recommended to read Suttan &amp; Burto’s book(introduction to reinforcement learning) or David Silver’s lecture.</p>
<h3 id="What-is-Reinforcment-Learning-RL"><a href="#What-is-Reinforcment-Learning-RL" class="headerlink" title="What is Reinforcment Learning(RL)?"></a>What is Reinforcment Learning(RL)?</h3><p>RL is one branch of machine learning. In contrast to other machine learning such as supervised and unsupervised learning, it is based on try and error method that collects data by interacting with environment also called world.</p>
<p>RL is built on Markov Decision Process(MDP) which means that next state only depends on current states(Markov Property). MDP in RL has 5 elements: states($S$), action($A$), transition probability($P$), discount factor($\gamma$), and reward($R$).</p>
<script type="math/tex; mode=display">P(s_{t+1}|s_{t},a_{t},s_{t-1},a_{t_1} ... )=P(s_{t+1} | s_t, a_t)</script><p>The Goal of RL is to find the policy that maximize cumulative(sum of) discounted rewards.<br>※ Sum of rewards depends on which problem you want to solve. Ex. one is episodic task, the other is infinite time horizon task.</p>
<script type="math/tex; mode=display">J(\pi) = E\Big[\sum_{t=0}^{T}\gamma^tr_t\Big]</script><h3 id="Value-Function-amp-Bellman-Equation"><a href="#Value-Function-amp-Bellman-Equation" class="headerlink" title="Value Function &amp; Bellman Equation"></a>Value Function &amp; Bellman Equation</h3><p>There are two kinds of value function. One is state value function($V(s)$) and the other is action value function($Q(s,a)$).<br>State value function denotes how much return(sum of rewards) would get from this state($s$).</p>
<script type="math/tex; mode=display">V(s) = E\Big[\sum_{k=0}^{T}\gamma^kr_{t+k}|s_t = s\Big]</script><p>Action value function means how much return would get from this state when choosing particular action($a$)</p>
<script type="math/tex; mode=display">Q(s,a) = E\Big[\sum_{k=0}^{T}\gamma^kr_{t+k}|s_t = s,a_t = a\Big]</script><p>We can also write the equation using dynamic programming which writes the equation in relationship between current and next.</p>
<script type="math/tex; mode=display">V(s) = E_{a\sim\pi}\Big[Q(s,a)\Big]</script><script type="math/tex; mode=display">V(s) = E_{a\sim\pi}\Big[E_{s'\sim P}\Big[r(s,a)+\gamma V(s')\Big]\Big]</script><script type="math/tex; mode=display">Q(s,a) = E_{s'\sim P}\Big[r(s,a)+\gamma V(s')\Big]</script><script type="math/tex; mode=display">Q(s,a) = E_{s'\sim P}\Big[r(s,a)+\gamma E_{a'\sim\pi}\Big[Q(s',a')\Big]\Big]</script><h3 id="Bellman-Optimality-Equation"><a href="#Bellman-Optimality-Equation" class="headerlink" title="Bellman Optimality Equation"></a>Bellman Optimality Equation</h3><p>Key idea is to choose the policy greedly that maximize value function. This means that policy value which is probability of choosing action at state is 1 for max value function.</p>
<script type="math/tex; mode=display">V(s) = E_{a\sim\pi}\Big[E_{s'\sim P}\Big[r(s,a)+\gamma V(s')\Big]\Big]</script><script type="math/tex; mode=display">\downarrow</script><script type="math/tex; mode=display">V^{*}(s) = \max_{a}\Big(E_{s'\sim P}\Big[r(s,a)+\gamma V^{*}(s')\Big]\Big)</script><script type="math/tex; mode=display">Q(s,a) = E_{s'\sim P}\Big[r(s,a)+\gamma E_{a'\sim\pi}\Big[Q(s',a')\Big]\Big]</script><script type="math/tex; mode=display">\downarrow</script><script type="math/tex; mode=display">Q^{*}(s,a) = E_{s'\sim P}\Big[r(s,a)+\gamma \max_a\Big(Q^{*}(s',a')\Big)\Big]</script><h3 id="Policy-Evaluation"><a href="#Policy-Evaluation" class="headerlink" title="Policy Evaluation"></a>Policy Evaluation</h3><ul>
<li>Use <strong>Bellman Expectation Equation</strong> to update current value function(state or action) by looking ahead</li>
<li>Value function indicates how good current policy is</li>
</ul>
<h3 id="Value-Iteration"><a href="#Value-Iteration" class="headerlink" title="Value Iteration"></a>Value Iteration</h3><ul>
<li>Use <strong>Bellman Optimality Equation</strong> to update current value function(state or action) by looking ahead</li>
<li>To find optimal policy and optimal value function</li>
<li>After few iterations, Value function is converge. We can use this value function as policy, which means choosing action that gives highest value function.</li>
<li>How to find optimal policy?<ul>
<li>For state value function, we can use optimality equation again to make $Q^{＊}(s,a)$<script type="math/tex; mode=display">\pi(a\vert s) = arg \max_{a}Q^{＊}(s,a) = arg\max_{a}\Big(r(s,a)+\gamma E_{s'\sim P}[V^{＊}(s')] \Big)</script></li>
<li>For action value function, we just use $Q^{＊}(s,a)$ to decide which action to take.<script type="math/tex; mode=display">\pi(a\vert s) = arg \max_{a}Q^{＊}(s,a)</script>※Convergence is guaranteed for discrete state and action case with discounted reward following contraction mapping theorem.</li>
</ul>
</li>
</ul>
<h3 id="Policy-Iteration"><a href="#Policy-Iteration" class="headerlink" title="Policy Iteration"></a>Policy Iteration</h3><ul>
<li>In contrast to Value Iteration, Policy Iteration chooses greedy action according to action value function at each iteration.</li>
<li>Algorithm<ul>
<li>for $1$ : $K$ iterate<ol>
<li>Estimate Value function using Bellman Equation(Policy Evaluation)<script type="math/tex; mode=display">Q(s,a) = E_{s'\sim P}\Big[r(s,a)+\gamma E_{a'\sim\pi}\Big[Q(s',a')\Big]\Big]</script></li>
<li>Choose action greedily<script type="math/tex; mode=display">\pi(a\vert s) = arg \max_{a}Q^{＊}(s,a)</script></li>
</ol>
</li>
</ul>
</li>
</ul>
<h3 id="Summary-amp-Limitation"><a href="#Summary-amp-Limitation" class="headerlink" title="Summary &amp; Limitation"></a>Summary &amp; Limitation</h3><p>Almost all of current SOTA in deep reinforcement learning is based on 3 basic algorithms(policy evaluation, value iteration, and policy iteration). For example, we can see Deep Q Network(DQN) as policy iteration. This is because after we collect samples from black box environment, DQN algorithm fits Q function to reduce bellman error(the sqaured error of Bellman Equation). Then, we choose eplison-greedy action based on updated Q function. However, there are limitations directly using above algorithms.</p>
<ul>
<li><p><strong>Unknown Transition Probability</strong><br>First, we do not know explicit transition probability. One way to address this problem is to use model based reinforcement learning, which it defines or makes model of transition probability. After that, we can predict or plan the trajectory(states and action sequence) based on model. Another popular method is model-free reinforcement learning, which it learns policy, value function, or both end-to-end manner from sample. This method does not care modeling transition probability. It assumes transition probability as black box model.</p>
</li>
<li><p><strong>Curse of Dimensionality</strong><br>In practice, most problems have high-dimensional action and state space. It is hard to deal with huge matrix information. For specific, when we have $N$ states and $M$ actions. We need large values for each components: $N^2M$ for transition probability, $NM$ for policy, $NM$ for policy, $NM$ for action value function, and $N$ for state value function. When state and action space are continuous, it is almost impossible to discretize the space. Therefore, we can not full back-up hole MDP tree or look ahead all state and action space. To deal with this, we use samples to estimate value function such as Monte Carlo estimation or Temporal Difference Learning(TD).</p>
</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://donghyunsung-ms.github.io/2020/06/20/markdown-and-Latex-test/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="Donghyun Sung">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Welcome to Donghyun's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/06/20/markdown-and-Latex-test/" class="post-title-link" itemprop="url">Markdown and Latex Test</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-06-20 23:45:06" itemprop="dateCreated datePublished" datetime="2020-06-20T23:45:06+09:00">2020-06-20</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-12-27 03:51:41" itemprop="dateModified" datetime="2020-12-27T03:51:41+09:00">2020-12-27</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Notes/" itemprop="url" rel="index"><span itemprop="name">Notes</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>This post is to check markdown and mathjax setting</p>
<h4 id="Markdown-Test"><a href="#Markdown-Test" class="headerlink" title="Markdown Test"></a>Markdown Test</h4><p>Title</p>
<h1 id="big"><a href="#big" class="headerlink" title="big"></a>big</h1><h2 id="big-1"><a href="#big-1" class="headerlink" title="big"></a>big</h2><h3 id="big-2"><a href="#big-2" class="headerlink" title="big"></a>big</h3><h4 id="big-3"><a href="#big-3" class="headerlink" title="big"></a>big</h4><h5 id="big-4"><a href="#big-4" class="headerlink" title="big"></a>big</h5><p>Code Block<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#python test</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> torch</span><br></pre></td></tr></table></figure><br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;fstream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&quot;Eigen/Core&quot;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&quot;ros.h&quot;</span></span></span><br></pre></td></tr></table></figure></p>
<h4 id="Latex-Test"><a href="#Latex-Test" class="headerlink" title="Latex Test"></a>Latex Test</h4><script type="math/tex; mode=display">A+B = C</script><script type="math/tex; mode=display">x = {-b \pm \sqrt{b^2-4ac} \over 2a}</script><p>\begin{pmatrix}<br> 1 &amp; a_1 &amp; a_1^2 &amp; \cdots &amp; a_1^n \\\\<br> 1 &amp; a_2 &amp; a_2^2 &amp; \cdots &amp; a_2^n \\\\<br> \vdots  &amp; \vdots&amp; \vdots &amp; \ddots &amp; \vdots \\\\<br> 1 &amp; a_m &amp; a_m^2 &amp; \cdots &amp; a_m^n<br> \end{pmatrix}</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Donghyun Sung"
      src="/images/avatar.jpg">
  <p class="site-author-name" itemprop="name">Donghyun Sung</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">6</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">11</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">4</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/DonghyunSung-MS" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;DonghyunSung-MS" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="/dh-sung@naver.com" title="E-Mail → dh-sung@naver.com"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Donghyun Sung</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  


</body>
</html>
