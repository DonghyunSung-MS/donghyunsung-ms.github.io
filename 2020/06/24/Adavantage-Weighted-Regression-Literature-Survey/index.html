<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"donghyunsung-ms.github.io","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"right","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="This post is to summary recent RL algorithm called Adavantage Weighted Regression(AWR) (paper). Detail derivation and explanation are added to help understand deeply.※Explanation may not be accurate.">
<meta property="og:type" content="article">
<meta property="og:title" content="Adavantage Weighted Regression-Literature Survey">
<meta property="og:url" content="https://donghyunsung-ms.github.io/2020/06/24/Adavantage-Weighted-Regression-Literature-Survey/index.html">
<meta property="og:site_name" content="Welcome to Donghyun&#39;s Blog">
<meta property="og:description" content="This post is to summary recent RL algorithm called Adavantage Weighted Regression(AWR) (paper). Detail derivation and explanation are added to help understand deeply.※Explanation may not be accurate.">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://donghyunsung-ms.github.io/2020/06/24/Adavantage-Weighted-Regression-Literature-Survey/algo.png">
<meta property="article:published_time" content="2020-06-24T07:32:57.000Z">
<meta property="article:modified_time" content="2020-07-30T09:50:13.000Z">
<meta property="article:author" content="Donghyun Sung">
<meta property="article:tag" content="Do it">
<meta property="article:tag" content="RL">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://donghyunsung-ms.github.io/2020/06/24/Adavantage-Weighted-Regression-Literature-Survey/algo.png">

<link rel="canonical" href="https://donghyunsung-ms.github.io/2020/06/24/Adavantage-Weighted-Regression-Literature-Survey/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>Adavantage Weighted Regression-Literature Survey | Welcome to Donghyun's Blog</title>
  
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-171013578-1"></script>
    <script>
      if (CONFIG.hostname === location.hostname) {
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'UA-171013578-1');
      }
    </script>






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Welcome to Donghyun's Blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">Research & Daily Life</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://donghyunsung-ms.github.io/2020/06/24/Adavantage-Weighted-Regression-Literature-Survey/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="Donghyun Sung">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Welcome to Donghyun's Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Adavantage Weighted Regression-Literature Survey
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-06-24 16:32:57" itemprop="dateCreated datePublished" datetime="2020-06-24T16:32:57+09:00">2020-06-24</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-07-30 18:50:13" itemprop="dateModified" datetime="2020-07-30T18:50:13+09:00">2020-07-30</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Reinforcement-Learning/" itemprop="url" rel="index"><span itemprop="name">Reinforcement Learning</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Reinforcement-Learning/Model-Free/" itemprop="url" rel="index"><span itemprop="name">Model-Free</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Reinforcement-Learning/Model-Free/Off-policy/" itemprop="url" rel="index"><span itemprop="name">Off-policy</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Reinforcement-Learning/Model-Free/Off-policy/AWR/" itemprop="url" rel="index"><span itemprop="name">AWR</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>This post is to summary recent RL algorithm called <strong>Adavantage Weighted Regression(AWR)</strong> <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1910.00177">(paper)</a>. Detail derivation and explanation are added to help understand deeply.<br>※Explanation may not be accurate. Readers should read this post carefully.</p>
<h3 id="Contribution-personal"><a href="#Contribution-personal" class="headerlink" title="Contribution(personal)"></a>Contribution(personal)</h3><ul>
<li>This algorithm improves Reward Weighted Regression by using policy improvement instead of direct policy maximization.</li>
<li>It is off-policy algorithm that has higher sample efficiency than on-policy one.</li>
<li>It can learn policy from static expert data set without collecting or sampling data from environment like behavior cloning.</li>
</ul>
<h3 id="Preliminaries"><a href="#Preliminaries" class="headerlink" title="Preliminaries"></a>Preliminaries</h3><p>As always, we want to find policy that maximize return(sum of discounted rewards). We can represent objective as time or state and action.</p>
<script type="math/tex; mode=display">J(\pi)=E_{\tau\sim p_{\pi}(\tau)}\Big[\sum_{t=0}^{\infty}\gamma^t r_t\Big]</script><script type="math/tex; mode=display">J(\pi)=\sum_{t=0}^{\infty}\gamma^t\int_{s}p(s_t = s\vert\pi)\int_{a}\pi(a\vert s)r(s,a)dads</script><script type="math/tex; mode=display">J(\pi)=\int_{s}\sum_{t=0}^{\infty}\gamma^tp(s_t = s\vert\pi)\int_{a}\pi(a\vert s)r(s,a)dads</script><script type="math/tex; mode=display">J(\pi)=\int_{s}d^{\pi}(s)\int_{a}\pi(a\vert s)r(s,a)dads</script><script type="math/tex; mode=display">J(\pi)=E_{s\sim d^{\pi}(s)}\Big[E_{a\sim\pi(a\vert s)}\Big[r(s,a)\Big]\Big]</script><h3 id="AWR-Objective-amp-Derivation"><a href="#AWR-Objective-amp-Derivation" class="headerlink" title="AWR Objective &amp; Derivation"></a>AWR Objective &amp; Derivation</h3><p>As mentioned in contribution section, AWR algorithm maximizes policy improvement. In this equation, it is impossible to get expectation under discounted state distribution following policy($\pi$). According to 2002(Sham Kakade and John Langford) and 2015(TRPO) paper, expectation under sampling policy is tractable and it is approximate of true policy improvement with small error term(boundness).</p>
<script type="math/tex; mode=display">\eta(\pi)=J(\pi)-J(\mu)</script><script type="math/tex; mode=display">\eta(\pi)=E_{\tau\sim p_{\pi}(\tau)}\Big[\sum_{t=0}^{\infty}\gamma^t r_t\Big]-E_{s_0}\Big[V^{\mu}(s_0)\Big]</script><script type="math/tex; mode=display">\eta(\pi)=E_{\tau\sim p_{\pi}(\tau)}\Big[\sum_{t=0}^{\infty}\gamma^t r_t-V^{\mu}(s_0)\Big]</script><script type="math/tex; mode=display">\eta(\pi)=E_{\tau\sim p_{\pi}(\tau)}\Big[\sum_{t=0}^{\infty}\gamma^t (r_t+V^{\mu}(s_{t+1})-V^{\mu}(s_t)\Big]</script><script type="math/tex; mode=display">\eta(\pi)=E_{\tau\sim p_{\pi}(\tau)}\Big[\sum_{t=0}^{\infty}\gamma^t A^\mu(s_t,a_t)\Big]</script><p>As same derivation in preliminaries section, we can get equation under state, action expectation.</p>
<script type="math/tex; mode=display">\eta(\pi)=E_{s\sim d^{\pi}(s)}\Big[E_{a\sim\pi(a\vert s)}\Big[A^\mu(s,a)\Big]\Big]</script><p>“The objective can be difficult to optimize due to the dependency between $d^{\pi}(s)$ and $\pi$, as well as the need to collect samples from $\pi$” - In the paper</p>
<script type="math/tex; mode=display">\hat{\eta}(\pi)=E_{s\sim d^{\mu}(s)}\Big[E_{a\sim\pi(a\vert s)}\Big[A^\mu(s,a)\Big]\Big]</script><p>we can consider this optimization problem as constrained policy search. This is because, according to early paper(2015 TRPO), $\hat{\eta}(\pi)$ is guarantee only when $\pi$ ans $\mu$ are closed enough(the closeness in probability is defined as KL-divergence).</p>
<script type="math/tex; mode=display">arg\max_{\pi}=\hat{\eta}(\pi)</script><script type="math/tex; mode=display">s.t\quad\int_s d^{\mu}(s)D_{KL}(\pi(\cdot|s)\vert\vert\mu(\cdot|s))ds\leq\epsilon</script><script type="math/tex; mode=display">\int_{a}\pi(a\vert s)da = 1</script><p>we can re-write this equation in soft constrained form using Langrangian.</p>
<script type="math/tex; mode=display">L(\pi,\beta,\alpha_s)=\hat{\eta}(\pi)+\beta (\epsilon-\int_s d^{\mu}(s)D_{KL}(\pi(\cdot|s)\vert\vert\mu(\cdot|s))ds)+\int_{a}\alpha_s(1-\pi(a\vert s))da</script><script type="math/tex; mode=display">\frac{\partial L(\pi,\beta,\alpha_s)}{\partial\pi(a\vert s)} = 0</script><script type="math/tex; mode=display">\pi^{*}=\frac{1}{Z(s)}\mu(a\vert s) \exp(\frac{1}{\beta}A^{\mu}(s,a))</script><p>where $Z(s)$ is normalized constant to make $\pi^{*}$ sum up to 1</p>
<p>we want $\pi$ close enough to $\pi^*$ in terms of KL-divergence. we can write this in to optimization problem.</p>
<script type="math/tex; mode=display">arg\min_{\pi}\int_{s}d^\mu (s)D_{KL}(\pi^*(\cdot|s)\vert\vert\pi(\cdot|s))ds</script><p>Following the definition of KL-divergence you can easily get under problem.</p>
<script type="math/tex; mode=display">arg\max_{\pi}\int_{s}d^\mu (s)\int_{a}\mu(a\vert s) \log\pi(a\vert s)\exp(\frac{1}{\beta}A^{\mu}(s,a))ds</script><h3 id="Off-policy-Learning-with-Experience-Replay"><a href="#Off-policy-Learning-with-Experience-Replay" class="headerlink" title="Off-policy Learning with Experience Replay"></a>Off-policy Learning with Experience Replay</h3><p>On-policy learning uses behavior policy(sampling policy) only at k-th iteration trajectory($\tau$) data, which is inefficient. This is because we throw away hole data that collected from previous iterations. Instead, AWR uses hole data in replay buffer($D$). However, state distribution and policy at each iteration are different. This makes expectation of current policy impossible. In AWR derivation, this algorithm considers samples from replay buffer as prior policy that mixture of 1~k iterations.</p>
<h3 id="Algorithm"><a href="#Algorithm" class="headerlink" title="Algorithm"></a>Algorithm</h3><img src="/2020/06/24/Adavantage-Weighted-Regression-Literature-Survey/algo.png" class="">
<ol>
<li>Start from random policy(ex.generated by initial weights in policy network).</li>
<li>At each iteration sample trajectory following current policy($\pi_k$).</li>
<li>Uniformly select $N$ samples from replay buffer($D$).</li>
<li>Update state value function($TD(\lambda)$ as target semi-gradient descent).</li>
<li>Update policy following upper objective.</li>
</ol>
<h3 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h3><p>In the <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1910.00177">paper</a>, there are sections that I do not cover in this post. For specific, the author(Jason Peng) is famous for “deepmimic” which agent learns agile skills from mocap data using RL. He suggests that AWR is better performance than Proximal Policy optimization and Reward Weighted Regression algorithm in terms of fast convergence when do motion imitation tasks. Also, AWR learns from fully static data that collects form expert. He also compares AWR with others that can learn or cloning expert policy in fully off-line manner.</p>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/Do-it/" rel="tag"># Do it</a>
              <a href="/tags/RL/" rel="tag"># RL</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2020/06/23/RL-Core/" rel="prev" title="RL Core">
      <i class="fa fa-chevron-left"></i> RL Core
    </a></div>
      <div class="post-nav-item">
    <a href="/2020/06/29/Trust-Region-Policy-Optimization-Literature-Survey/" rel="next" title="Trust Region Policy Optimization-Literature Survey">
      Trust Region Policy Optimization-Literature Survey <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          
    <div class="comments" id="gitalk-container"></div>

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#Contribution-personal"><span class="nav-number">1.</span> <span class="nav-text">Contribution(personal)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Preliminaries"><span class="nav-number">2.</span> <span class="nav-text">Preliminaries</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#AWR-Objective-amp-Derivation"><span class="nav-number">3.</span> <span class="nav-text">AWR Objective &amp; Derivation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Off-policy-Learning-with-Experience-Replay"><span class="nav-number">4.</span> <span class="nav-text">Off-policy Learning with Experience Replay</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Algorithm"><span class="nav-number">5.</span> <span class="nav-text">Algorithm</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Summary"><span class="nav-number">6.</span> <span class="nav-text">Summary</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Donghyun Sung"
      src="/images/avatar.jpg">
  <p class="site-author-name" itemprop="name">Donghyun Sung</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">6</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">11</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">4</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/DonghyunSung-MS" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;DonghyunSung-MS" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="/dh-sung@naver.com" title="E-Mail → dh-sung@naver.com"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Donghyun Sung</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

<link rel="stylesheet" href="//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.css">

<script>
NexT.utils.loadComments(document.querySelector('#gitalk-container'), () => {
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js', () => {
    var gitalk = new Gitalk({
      clientID    : 'a7e410fac468f5b553b9',
      clientSecret: '9b77cc9a4bfb24c5b1523a1774adfa2f93eb410a',
      repo        : 'donghyunsung-ms.github.io',
      owner       : 'DonghyunSung-MS',
      admin       : ['DonghyunSung-MS'],
      id          : 'c247c319bcc79e4e3b57089ffbf9f104',
        language: 'en',
      distractionFreeMode: true
    });
    gitalk.render('gitalk-container');
  }, window.Gitalk);
});
</script>

</body>
</html>
