<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"donghyunsung-ms.github.io","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"right","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="This post is to introduce the key concept of reinforcement learning with math. When you want to read and understand recent RL papers, it is sometimes hard to follow the equation under expectation. I h">
<meta property="og:type" content="article">
<meta property="og:title" content="RL Core">
<meta property="og:url" content="https://donghyunsung-ms.github.io/2020/06/23/RL-Core/index.html">
<meta property="og:site_name" content="Welcome to Donghyun&#39;s Blog">
<meta property="og:description" content="This post is to introduce the key concept of reinforcement learning with math. When you want to read and understand recent RL papers, it is sometimes hard to follow the equation under expectation. I h">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2020-06-23T05:06:49.000Z">
<meta property="article:modified_time" content="2020-07-30T09:50:33.000Z">
<meta property="article:author" content="Donghyun Sung">
<meta property="article:tag" content="Do it">
<meta property="article:tag" content="RL">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://donghyunsung-ms.github.io/2020/06/23/RL-Core/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>RL Core | Welcome to Donghyun's Blog</title>
  
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-171013578-1"></script>
    <script>
      if (CONFIG.hostname === location.hostname) {
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'UA-171013578-1');
      }
    </script>






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Welcome to Donghyun's Blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">Research & Daily Life</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://donghyunsung-ms.github.io/2020/06/23/RL-Core/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="Donghyun Sung">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Welcome to Donghyun's Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          RL Core
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-06-23 14:06:49" itemprop="dateCreated datePublished" datetime="2020-06-23T14:06:49+09:00">2020-06-23</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-07-30 18:50:33" itemprop="dateModified" datetime="2020-07-30T18:50:33+09:00">2020-07-30</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Reinforcement-Learning/" itemprop="url" rel="index"><span itemprop="name">Reinforcement Learning</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Reinforcement-Learning/Intro/" itemprop="url" rel="index"><span itemprop="name">Intro</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>This post is to introduce the key concept of reinforcement learning with math. When you want to read and understand recent RL papers, it is sometimes hard to follow the equation under expectation. I hope that this article paves the way to deeper insight towards RL. If you are interested in this topic, it is recommended to read Suttan &amp; Burto’s book(introduction to reinforcement learning) or David Silver’s lecture.</p>
<h3 id="What-is-Reinforcment-Learning-RL"><a href="#What-is-Reinforcment-Learning-RL" class="headerlink" title="What is Reinforcment Learning(RL)?"></a>What is Reinforcment Learning(RL)?</h3><p>RL is one branch of machine learning. In contrast to other machine learning such as supervised and unsupervised learning, it is based on try and error method that collects data by interacting with environment also called world.</p>
<p>RL is built on Markov Decision Process(MDP) which means that next state only depends on current states(Markov Property). MDP in RL has 5 elements: states($S$), action($A$), transition probability($P$), discount factor($\gamma$), and reward($R$).</p>
<script type="math/tex; mode=display">P(s_{t+1}|s_{t},a_{t},s_{t-1},a_{t_1} ... )=P(s_{t+1} | s_t, a_t)</script><p>The Goal of RL is to find the policy that maximize cumulative(sum of) discounted rewards.<br>※ Sum of rewards depends on which problem you want to solve. Ex. one is episodic task, the other is infinite time horizon task.</p>
<script type="math/tex; mode=display">J(\pi) = E\Big[\sum_{t=0}^{T}\gamma^tr_t\Big]</script><h3 id="Value-Function-amp-Bellman-Equation"><a href="#Value-Function-amp-Bellman-Equation" class="headerlink" title="Value Function &amp; Bellman Equation"></a>Value Function &amp; Bellman Equation</h3><p>There are two kinds of value function. One is state value function($V(s)$) and the other is action value function($Q(s,a)$).<br>State value function denotes how much return(sum of rewards) would get from this state($s$).</p>
<script type="math/tex; mode=display">V(s) = E\Big[\sum_{k=0}^{T}\gamma^kr_{t+k}|s_t = s\Big]</script><p>Action value function means how much return would get from this state when choosing particular action($a$)</p>
<script type="math/tex; mode=display">Q(s,a) = E\Big[\sum_{k=0}^{T}\gamma^kr_{t+k}|s_t = s,a_t = a\Big]</script><p>We can also write the equation using dynamic programming which writes the equation in relationship between current and next.</p>
<script type="math/tex; mode=display">V(s) = E_{a\sim\pi}\Big[Q(s,a)\Big]</script><script type="math/tex; mode=display">V(s) = E_{a\sim\pi}\Big[E_{s'\sim P}\Big[r(s,a)+\gamma V(s')\Big]\Big]</script><script type="math/tex; mode=display">Q(s,a) = E_{s'\sim P}\Big[r(s,a)+\gamma V(s')\Big]</script><script type="math/tex; mode=display">Q(s,a) = E_{s'\sim P}\Big[r(s,a)+\gamma E_{a'\sim\pi}\Big[Q(s',a')\Big]\Big]</script><h3 id="Bellman-Optimality-Equation"><a href="#Bellman-Optimality-Equation" class="headerlink" title="Bellman Optimality Equation"></a>Bellman Optimality Equation</h3><p>Key idea is to choose the policy greedly that maximize value function. This means that policy value which is probability of choosing action at state is 1 for max value function.</p>
<script type="math/tex; mode=display">V(s) = E_{a\sim\pi}\Big[E_{s'\sim P}\Big[r(s,a)+\gamma V(s')\Big]\Big]</script><script type="math/tex; mode=display">\downarrow</script><script type="math/tex; mode=display">V^{*}(s) = \max_{a}\Big(E_{s'\sim P}\Big[r(s,a)+\gamma V^{*}(s')\Big]\Big)</script><script type="math/tex; mode=display">Q(s,a) = E_{s'\sim P}\Big[r(s,a)+\gamma E_{a'\sim\pi}\Big[Q(s',a')\Big]\Big]</script><script type="math/tex; mode=display">\downarrow</script><script type="math/tex; mode=display">Q^{*}(s,a) = E_{s'\sim P}\Big[r(s,a)+\gamma \max_a\Big(Q^{*}(s',a')\Big)\Big]</script><h3 id="Policy-Evaluation"><a href="#Policy-Evaluation" class="headerlink" title="Policy Evaluation"></a>Policy Evaluation</h3><ul>
<li>Use <strong>Bellman Expectation Equation</strong> to update current value function(state or action) by looking ahead</li>
<li>Value function indicates how good current policy is</li>
</ul>
<h3 id="Value-Iteration"><a href="#Value-Iteration" class="headerlink" title="Value Iteration"></a>Value Iteration</h3><ul>
<li>Use <strong>Bellman Optimality Equation</strong> to update current value function(state or action) by looking ahead</li>
<li>To find optimal policy and optimal value function</li>
<li>After few iterations, Value function is converge. We can use this value function as policy, which means choosing action that gives highest value function.</li>
<li>How to find optimal policy?<ul>
<li>For state value function, we can use optimality equation again to make $Q^{＊}(s,a)$<script type="math/tex; mode=display">\pi(a\vert s) = arg \max_{a}Q^{＊}(s,a) = arg\max_{a}\Big(r(s,a)+\gamma E_{s'\sim P}[V^{＊}(s')] \Big)</script></li>
<li>For action value function, we just use $Q^{＊}(s,a)$ to decide which action to take.<script type="math/tex; mode=display">\pi(a\vert s) = arg \max_{a}Q^{＊}(s,a)</script>※Convergence is guaranteed for discrete state and action case with discounted reward following contraction mapping theorem.</li>
</ul>
</li>
</ul>
<h3 id="Policy-Iteration"><a href="#Policy-Iteration" class="headerlink" title="Policy Iteration"></a>Policy Iteration</h3><ul>
<li>In contrast to Value Iteration, Policy Iteration chooses greedy action according to action value function at each iteration.</li>
<li>Algorithm<ul>
<li>for $1$ : $K$ iterate<ol>
<li>Estimate Value function using Bellman Equation(Policy Evaluation)<script type="math/tex; mode=display">Q(s,a) = E_{s'\sim P}\Big[r(s,a)+\gamma E_{a'\sim\pi}\Big[Q(s',a')\Big]\Big]</script></li>
<li>Choose action greedily<script type="math/tex; mode=display">\pi(a\vert s) = arg \max_{a}Q^{＊}(s,a)</script></li>
</ol>
</li>
</ul>
</li>
</ul>
<h3 id="Summary-amp-Limitation"><a href="#Summary-amp-Limitation" class="headerlink" title="Summary &amp; Limitation"></a>Summary &amp; Limitation</h3><p>Almost all of current SOTA in deep reinforcement learning is based on 3 basic algorithms(policy evaluation, value iteration, and policy iteration). For example, we can see Deep Q Network(DQN) as policy iteration. This is because after we collect samples from black box environment, DQN algorithm fits Q function to reduce bellman error(the sqaured error of Bellman Equation). Then, we choose eplison-greedy action based on updated Q function. However, there are limitations directly using above algorithms.</p>
<ul>
<li><p><strong>Unknown Transition Probability</strong><br>First, we do not know explicit transition probability. One way to address this problem is to use model based reinforcement learning, which it defines or makes model of transition probability. After that, we can predict or plan the trajectory(states and action sequence) based on model. Another popular method is model-free reinforcement learning, which it learns policy, value function, or both end-to-end manner from sample. This method does not care modeling transition probability. It assumes transition probability as black box model.</p>
</li>
<li><p><strong>Curse of Dimensionality</strong><br>In practice, most problems have high-dimensional action and state space. It is hard to deal with huge matrix information. For specific, when we have $N$ states and $M$ actions. We need large values for each components: $N^2M$ for transition probability, $NM$ for policy, $NM$ for policy, $NM$ for action value function, and $N$ for state value function. When state and action space are continuous, it is almost impossible to discretize the space. Therefore, we can not full back-up hole MDP tree or look ahead all state and action space. To deal with this, we use samples to estimate value function such as Monte Carlo estimation or Temporal Difference Learning(TD).</p>
</li>
</ul>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/Do-it/" rel="tag"># Do it</a>
              <a href="/tags/RL/" rel="tag"># RL</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2020/06/20/markdown-and-Latex-test/" rel="prev" title="Markdown and Latex Test">
      <i class="fa fa-chevron-left"></i> Markdown and Latex Test
    </a></div>
      <div class="post-nav-item">
    <a href="/2020/06/24/Adavantage-Weighted-Regression-Literature-Survey/" rel="next" title="Adavantage Weighted Regression-Literature Survey">
      Adavantage Weighted Regression-Literature Survey <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          
    <div class="comments" id="gitalk-container"></div>

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#What-is-Reinforcment-Learning-RL"><span class="nav-number">1.</span> <span class="nav-text">What is Reinforcment Learning(RL)?</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Value-Function-amp-Bellman-Equation"><span class="nav-number">2.</span> <span class="nav-text">Value Function &amp; Bellman Equation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Bellman-Optimality-Equation"><span class="nav-number">3.</span> <span class="nav-text">Bellman Optimality Equation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Policy-Evaluation"><span class="nav-number">4.</span> <span class="nav-text">Policy Evaluation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Value-Iteration"><span class="nav-number">5.</span> <span class="nav-text">Value Iteration</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Policy-Iteration"><span class="nav-number">6.</span> <span class="nav-text">Policy Iteration</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Summary-amp-Limitation"><span class="nav-number">7.</span> <span class="nav-text">Summary &amp; Limitation</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Donghyun Sung"
      src="/images/avatar.jpg">
  <p class="site-author-name" itemprop="name">Donghyun Sung</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">6</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">11</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">4</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/DonghyunSung-MS" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;DonghyunSung-MS" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="/dh-sung@naver.com" title="E-Mail → dh-sung@naver.com"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Donghyun Sung</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

<link rel="stylesheet" href="//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.css">

<script>
NexT.utils.loadComments(document.querySelector('#gitalk-container'), () => {
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js', () => {
    var gitalk = new Gitalk({
      clientID    : 'a7e410fac468f5b553b9',
      clientSecret: '9b77cc9a4bfb24c5b1523a1774adfa2f93eb410a',
      repo        : 'donghyunsung-ms.github.io',
      owner       : 'DonghyunSung-MS',
      admin       : ['DonghyunSung-MS'],
      id          : '840d4c90b752014c7e433ea4f204f5c8',
        language: 'en',
      distractionFreeMode: true
    });
    gitalk.render('gitalk-container');
  }, window.Gitalk);
});
</script>

</body>
</html>
