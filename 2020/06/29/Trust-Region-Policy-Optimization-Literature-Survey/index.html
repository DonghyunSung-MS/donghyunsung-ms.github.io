<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"donghyunsung-ms.github.io","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"right","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="Trust Region Policy Optimization[paper] is on-policy learning with giving constraint on policy. There are a lot of method to address high-dimensional space decision problem in robotics and game domain">
<meta property="og:type" content="article">
<meta property="og:title" content="Trust Region Policy Optimization-Literature Survey">
<meta property="og:url" content="https://donghyunsung-ms.github.io/2020/06/29/Trust-Region-Policy-Optimization-Literature-Survey/index.html">
<meta property="og:site_name" content="Welcome to Donghyun&#39;s Blog">
<meta property="og:description" content="Trust Region Policy Optimization[paper] is on-policy learning with giving constraint on policy. There are a lot of method to address high-dimensional space decision problem in robotics and game domain">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2020-06-29T08:36:04.000Z">
<meta property="article:modified_time" content="2020-12-26T18:56:16.500Z">
<meta property="article:author" content="Donghyun Sung">
<meta property="article:tag" content="RL">
<meta property="article:tag" content="Do it">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://donghyunsung-ms.github.io/2020/06/29/Trust-Region-Policy-Optimization-Literature-Survey/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>Trust Region Policy Optimization-Literature Survey | Welcome to Donghyun's Blog</title>
  
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-171013578-1"></script>
    <script>
      if (CONFIG.hostname === location.hostname) {
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'UA-171013578-1');
      }
    </script>






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Welcome to Donghyun's Blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">Research & Daily Life</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://donghyunsung-ms.github.io/2020/06/29/Trust-Region-Policy-Optimization-Literature-Survey/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="Donghyun Sung">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Welcome to Donghyun's Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Trust Region Policy Optimization-Literature Survey
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-06-29 17:36:04" itemprop="dateCreated datePublished" datetime="2020-06-29T17:36:04+09:00">2020-06-29</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-12-27 03:56:16" itemprop="dateModified" datetime="2020-12-27T03:56:16+09:00">2020-12-27</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Reinforcement-Learning/" itemprop="url" rel="index"><span itemprop="name">Reinforcement Learning</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Reinforcement-Learning/Model-Free/" itemprop="url" rel="index"><span itemprop="name">Model-Free</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Reinforcement-Learning/Model-Free/On-policy/" itemprop="url" rel="index"><span itemprop="name">On-policy</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Reinforcement-Learning/Model-Free/On-policy/TRPO/" itemprop="url" rel="index"><span itemprop="name">TRPO</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>Trust Region Policy Optimization[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1502.05477">paper</a>] is on-policy learning with giving constraint on policy. There are a lot of method to address high-dimensional space decision problem in robotics and game domain. The main idea is using function approximation instead of matrix or table. Even though policy gradient methods have been succeeded, it can not improve policy efficiently.<br>The following post illustrates 2002 [<a href="chrome-extension://ohfgljdgelakfkefopgklcohadegdpjf/https://www.cs.cmu.edu/~./jcl/papers/aoarl/Final.pdf">paper</a>] first and then covers TRPO paper.</p>
<h3 id="Policy-Improvement-2002-rightarrow-2015"><a href="#Policy-Improvement-2002-rightarrow-2015" class="headerlink" title="Policy Improvement(2002 $\rightarrow$ 2015)"></a>Policy Improvement(2002 $\rightarrow$ 2015)</h3><p>The concepts of both paper are similar. Both paper focus on policy improvement rather than maximize policy objective itself. This formulation guarantees steady improvement in 2002 paper. Also, they prove the boundness and effectiveness of surrogate objective function with theoretical and experimental result.</p>
<h4 id="Policy-Objective-surrogate-objective"><a href="#Policy-Objective-surrogate-objective" class="headerlink" title="Policy Objective(surrogate objective)"></a>Policy Objective(surrogate objective)</h4><ul>
<li><strong>In time repersentation</strong><script type="math/tex; mode=display">\eta(\pi) = E_{s_0,a_0,s_1,...}[\sum_{t=0}^{\infty}\gamma^{t}r(s_{t})]</script></li>
<li><strong>In state representation</strong></li>
</ul>
<p>$\eta(\tilde\pi) = E<em>{\tau\sim\tilde\pi}[\sum</em>{t=0}^{\infty}\gamma^{t}r(s<em>{t})]$, $\,\,\eta(\pi) = E</em>{\tau\sim\pi}[\sum<em>{t=0}^{\infty}\gamma^{t}r(s</em>{t})]$</p>
<p>$\eta(\tilde\pi) = \eta(\pi)-E<em>{\tau\sim\pi}[\sum</em>{t=0}^{\infty}\gamma^{t}r(s<em>{t})] + E</em>{\tau\sim\tilde\pi}[\sum<em>{t=0}^{\infty}\gamma^{t}r(s</em>{t})]$</p>
<p>$\eta(\tilde\pi) = \eta(\pi)-V<em>{\pi}(s_0) + E</em>{\tau\sim\tilde\pi}[\sum<em>{t=0}^{\infty}\gamma^{t}r(s</em>{t})]$</p>
<p>$\eta(\tilde\pi) = \eta(\pi) + E<em>{\tau\sim\tilde\pi}[-V</em>{\pi}(s<em>0)+\sum</em>{t=0}^{\infty}\gamma^{t}r(s_{t})]$</p>
<p>$\eta(\tilde\pi) = \eta(\pi) + E<em>{\tau\sim\tilde\pi}[-V</em>{\pi}(s<em>0)+r(s_0)+\gamma V</em>{\pi}(s<em>1)+\gamma{-V</em>{\pi}(s<em>1)+r(s_1)+\gamma V</em>{\pi}(s_2)}…]$</p>
<p>$\eta(\tilde\pi) = \eta(\pi) + E<em>{\tau\sim\tilde\pi}[\sum</em>{t=0}^{\infty}\gamma^t A_{\pi}(s_t,a_t)]$</p>
<p>$\eta(\tilde\pi) = \eta(\pi) + \sum<em>{t=0}^{\infty}\gamma^t\sum_sP(s_t=s;\tilde\pi)\sum_a\tilde\pi(a_t|s_t) A</em>{\pi}(s_t,a_t)$</p>
<p><strong>Result</strong></p>
<script type="math/tex; mode=display">\eta(\tilde\pi) = \eta(\pi)+\sum_{s}\rho_{\tilde\pi}(s)\sum_{a}\tilde\pi(a|s)A_{\pi}(s,a)</script><p>It is hard to formulate equation under $\rho_{\tilde\pi}(s)$. In general policy iteration method, policy evaluation is followed by policy improvenment. After improvement, the agent has not experienced or rolled out with new policy so that new discounted unnormalized visitation frequency has not formed yet. The main idea in 2002 and 2015 literature was, instead of using next policy state disturibution(unnormalized), using previous one.</p>
<script type="math/tex; mode=display">L_{\pi}(\tilde\pi) = \eta(\pi)+\sum_{s}\rho_{\pi}(s)\sum_{a}\tilde\pi(a|s)A_{\pi}(s,a)</script><script type="math/tex; mode=display">\pi'\in arg\max_{\pi'}L_{\pi}(\pi')</script><script type="math/tex; mode=display">\pi_{new}=(1-\alpha){\pi_{old}}+\alpha{\pi'}</script><p>They suggest that it can not gaurantee direct maximizing the advantage function is equal to improvement in policy. This is because the advantage in practice is parameterized, which causes estimation error and approximation error at the same time. Instead, they use conservative policy iteration update so that they could provide explicit lower bounds on the improvement of η.</p>
<h5 id="Boundness"><a href="#Boundness" class="headerlink" title="Boundness"></a><strong>Boundness</strong></h5><p>Let’s go with 2002 approach before we dive into 2015 approach which little bit changes in policy improvement.</p>
<ul>
<li><strong>Properties &amp; Condition</strong></li>
</ul>
<p>$\eta(\tilde\pi) = \eta(\pi) + E<em>{\tau\sim\tilde\pi}[\sum</em>{t=0}^{\infty}\gamma^t A_{\pi}(s_t,a_t)]$</p>
<p>If $\tilde\pi=\pi$</p>
<p>$E<em>{\tau\sim\pi}[\sum</em>{t=0}^{\infty}\gamma^t A_{\pi}(s_t,a_t)]=0$</p>
<p>$\sum<em>{s}\rho</em>{\pi}(s)\sum<em>{a}\pi(a|s)A</em>{\pi}(s,a)=0$</p>
<p>$\sum<em>{a}\pi(a|s)A</em>{\pi}(s,a)=0$  </p>
<p>$\epsilon<em>{old} = \max_s|E</em>{a\sim\pi’}A<em>{\pi}(s,a)|\geq |E</em>{a\sim\pi’}A_{\pi}(s,a)|$  </p>
<ul>
<li><strong>Derivation</strong></li>
</ul>
<p>$\eta(\pi<em>{new}) = \eta(\pi</em>{old}) + E<em>{\tau\sim\pi</em>{new}}[\sum<em>{t=0}^{\infty}\gamma^t A</em>{\pi}(s<em>t,a_t)]$<br>$\eta(\pi</em>{new}) = \eta(\pi<em>{old}) + \sum</em>{s}\rho<em>{\pi</em>{new}}(s)\sum<em>{a}\pi</em>{new}(a|s)A<em>{\pi</em>{old}}(s,a)$<br>$\eta(\pi<em>{new}) = \eta(\pi</em>{old}) + \sum<em>{s}\rho</em>{\pi<em>{new}}(s)\sum</em>{a}{(1-\alpha)\pi<em>{old}(a|s)+\alpha\pi’(a|s)}A</em>{\pi<em>{old}}(s,a)$<br>$\eta(\pi</em>{new}) = \eta(\pi<em>{old}) + \sum</em>{s}\rho<em>{\pi</em>{new}}(s)\sum<em>{a}{\alpha\pi’(a|s)}A</em>{\pi<em>{old}}(s,a)$<br>$\eta(\pi</em>{new}) = \eta(\pi<em>{old}) + \sum</em>{t=0}^{\infty}\gamma^t\sum<em>sP(s_t=s;\pi</em>{new})\sum<em>{a}{\alpha\pi’(a|s)}A</em>{\pi<em>{old}}(s,a)$<br>$\eta(\pi</em>{new}) = \eta(\pi<em>{old}) + \sum</em>{t=0}^{\infty}\gamma^t\sum<em>s{(1-\alpha)^t P(s_t=s;\pi</em>{old\,only})+(1-(1-\alpha)^t) P(s<em>t=s;\pi</em>{rest})}\sum<em>{a}{\alpha\pi’(a|s)}A</em>{\pi_{old}}(s,a)$<br>Let $r_a$ denotes $1-(1-\alpha)^t$,</p>
<p>$\eta(\pi<em>{new}) = \eta(\pi</em>{old}) + \sum<em>{t=0}^{\infty}\gamma^t\sum_s{(1-r_a) P(s_t=s;\pi</em>{old\,only})+r<em>a P(s_t=s;\pi</em>{rest})}\sum<em>{a}{\alpha\pi’(a|s)}A</em>{\pi<em>{old}}(s,a)$<br>$\eta(\pi</em>{new}) = L<em>{\pi</em>{old}}(\pi<em>{new}) + \sum</em>{t=0}^{\infty}\gamma^t\sum<em>s{-r_a P(s_t=s;\pi</em>{old\,only})+r<em>a P(s_t=s;\pi</em>{rest})}\sum<em>{a}{\alpha\pi’(a|s)}A</em>{\pi<em>{old}}(s,a)$<br>$\eta(\pi</em>{new}) = L<em>{\pi</em>{old}}(\pi<em>{new}) - \sum</em>{t=0}^{\infty}\gamma^t2r<em>{a}\epsilon</em>{old}$</p>
<script type="math/tex; mode=display">\eta(\pi_{new}) \geq L_{\pi_{old}}(\pi_{new}) -\frac{2\alpha^2\epsilon_{old}\gamma}{(1-\gamma)^2}</script><p>This inequality condition means that if we maximize $L<em>{\pi</em>{old}}(\pi_{new})$, it gaurantees policy improvements with error term.</p>
<p>Let’s go with 2015 approach. They changes $\epsilon$ definition and give more generality while having more error term. We denotes $\epsilon$ as $\epsilon_{new}$</p>
<p>$\epsilon<em>{old} = \max</em>{s}\vert E<em>{a\sim\pi’} A</em>{\pi}(s,a)\vert = \max<em>s\vert\sum_a\pi(a|s)A</em>{\pi}(s,a)-\sum<em>a\pi’(a|s)A</em>{\pi}(s,a)\vert<br>\leq 2*\max<em>{s,a}\vert A</em>{\pi}(s,a) \vert =2\epsilon_{new}$</p>
<script type="math/tex; mode=display">\eta(\pi_{new}) \geq L_{\pi_{old}}(\pi_{new}) -\frac{4\alpha^2\epsilon_{new}\gamma}{(1-\gamma)^2}</script><script type="math/tex; mode=display">\eta(\pi_{new}) \geq L_{\pi_{old}}(\pi_{new}) -C * D_{KL}^{max}(\pi_{new}||\pi_{old})</script><script type="math/tex; mode=display">where\,\, C\,=\, \frac{4\epsilon_{new}\gamma}{(1-\gamma)^2},\,\,\alpha\,=\,D_{TV}^{max}(\pi_{new}||\pi_{old})</script><h5 id="Toward-Theory-to-Practical-Implementation-form"><a href="#Toward-Theory-to-Practical-Implementation-form" class="headerlink" title="Toward Theory to Practical Implementation form"></a>Toward Theory to Practical Implementation form</h5><ul>
<li><strong>Change Boundness to Constant</strong><br>In practice, policy($\pi$) is parameteriezed by $\theta$. If we follow theoretical step($C$), it would be small. The literature recommands to choose $\delta$, which changes the optimization problem.<script type="math/tex; mode=display">maximize_{\theta}\:L_{\theta_{old}}(\theta)</script><script type="math/tex; mode=display">subject\:to\:D_{KL}^{max}(\theta||\theta_{old})\leq\delta</script></li>
<li><strong>Exact method to Heuristic approximation</strong><br>It is impractical to calculate $D<em>{KL}^{max}$ at each iteration. Instead, they choose heuristic approximation($D</em>{KL}^{\rho}=E[D_{KL}]$)<script type="math/tex; mode=display">maximize_{\theta}\:L_{\theta_{old}}(\theta)</script><script type="math/tex; mode=display">subject\:to\:D_{KL}^{\rho}(\theta||\theta_{old})\leq\delta</script></li>
<li><strong>Expectation becomes Sampled Sum</strong><br>In this section, sampled sum which we call Monte-Carlo simulation replaces expecation  </li>
</ul>
<p>$L<em>{\pi}(\pi</em>{new}) = \eta(\pi<em>{old})+\sum</em>{s}\rho<em>{\pi</em>{old}}(s)\sum<em>{a}\pi</em>{new}(a|s)A<em>{\pi</em>{old}}(s,a)$</p>
<p>$\sum<em>{s}\rho</em>{\pi}(s)\:\rightarrow\:\frac{1}{1-\gamma}E<em>{s\sim\rho</em>{old}}[*]$<br>$A<em>{\pi</em>{old}}(s,a)\:\rightarrow\:Q<em>{\pi</em>{old}}(s,a)$</p>
<p>Importance of sampling($q$ sampling distribution)</p>
<p>$\sum<em>{a}\pi</em>{new}(a|s)A<em>{\pi</em>{old}}(s,a)\:\rightarrow\:E<em>{a\sim q}[\frac{\pi</em>{new}}{q}A<em>{\pi</em>{old}}(s,a)]$</p>
<script type="math/tex; mode=display">maximize_{\theta}\:E_{a\sim q,\,s\sim \rho_{old}}g[\frac{\pi_{new}}{q}Q_{\pi_{old}}(s,a)g]</script><script type="math/tex; mode=display">subject\:to\:D_{KL}^{\rho}(\theta||\theta_{old})\leq\delta</script><ul>
<li><strong>Linear Approximation of Objective Function, Quadratic Approximation of Constraint</strong>  </li>
</ul>
<script type="math/tex; mode=display">maximize_{\theta}\:\nabla L \Delta\theta</script><script type="math/tex; mode=display">subject\:to\:\frac{1}{2}\Delta\theta^T A \Delta\theta\leq\delta</script><p>where, $\:A\,$ is Fisherman Information Matrix(FIM), also Hessian of $D_{KL}$</p>
<h3 id="Details-Optional"><a href="#Details-Optional" class="headerlink" title="Details(Optional)"></a>Details(Optional)</h3><h4 id="Lagragian-Method"><a href="#Lagragian-Method" class="headerlink" title="Lagragian Method"></a>Lagragian Method</h4><script type="math/tex; mode=display">F(\Delta\theta,\,\lambda)\,=\,\nabla L^T\,\Delta\theta\,-\,\lambda(\frac{1}{2}\Delta\theta^TA \Delta\theta\,-\,\delta)</script><script type="math/tex; mode=display">\nabla F(\Delta\theta,\,\lambda)=0</script><p>This yields to two equation</p>
<script type="math/tex; mode=display">\nabla L - \lambda A \Delta\theta=0  ---- (1)</script><script type="math/tex; mode=display">\Delta\theta=\frac 1 \lambda A^{-1}\nabla L</script><script type="math/tex; mode=display">\lambda(\frac{1}{2}\Delta\theta^T A \Delta\theta-\delta)=0  ---- (2)</script><p>By KKT condition, $\lambda$ should be positive and $\delta$ should be $\frac{1}{2}\Delta\theta^T A \Delta\theta$.<br>Use second condition and equation 1</p>
<p>$\delta=\frac{1}{2}\Delta\theta^T A \Delta\theta=\frac{1}{2\lambda^2}\nabla L^T A^{-1}\nabla L$</p>
<p>$\frac{1}{\lambda} = \sqrt{\frac{2\delta}{\nabla L^T A^{-1}\nabla L}}$</p>
<p>Finally, we get update law and maximum value</p>
<p>$\Delta\theta = \theta-\theta_{old}= {\sqrt{\frac{2\delta}{\nabla L^T A^{-1}\nabla L}}}A^{-1}\nabla L $</p>
<p>$A^{-1}\nabla L $ denotes <strong>search direction</strong>.</p>
<p>${\sqrt{\frac{2\delta}{\nabla L^T A^{-1}\nabla L}}}$ indicates <strong>step size</strong>.</p>
<p>$ \nabla L^T \Delta\theta = {\sqrt{\frac{2\delta}{\nabla L^T A^{-1}\nabla L}}}{\nabla L^T A^{-1}\nabla L}$</p>
<h4 id="Conjugate-Gradient-Method"><a href="#Conjugate-Gradient-Method" class="headerlink" title="Conjugate Gradient Method"></a>Conjugate Gradient Method</h4><p>This is iterative method to solve $Ax=b$ or $minimize_{x}\: \Phi(x)=\frac{1}{2}x^TAx-b^Tx$.<br>where, $A$ is positive definite.<br>In previous section, we need $A^{-1}\nabla L$ to calculate update law. However, it is comuputationally high to get inverse of hessian($A^{-1}$). Instead, we compute hessian vector product iteratively.<br>In this case $Ax=g$, where $g$ is gradient of objective function($\nabla L$)</p>
<h5 id="Derivation-One-Minute-Derivation-of-The-Conjugate-Gradient-Algorithm"><a href="#Derivation-One-Minute-Derivation-of-The-Conjugate-Gradient-Algorithm" class="headerlink" title="Derivation(One-Minute Derivation of The Conjugate Gradient Algorithm)"></a>Derivation(<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1608.08691">One-Minute Derivation of The Conjugate Gradient Algorithm</a>)</h5><p>we want to solve for $Ax^＊=g$</p>
<p>$x<em>{i+1}=x</em>{i}+\alpha_i d_i$</p>
<p>$x<em>{i+1}-x^＊= x</em>{i}-x^＊+\alpha_i d_i$</p>
<p>Let, $e_i=x_i-x^＊$</p>
<p>$e<em>{i+1} = e</em>{i}+\alpha_i d_i$</p>
<p>$Ae<em>{i+1} = Ae</em>{i}+\alpha_i Ad_i$</p>
<p>Let, $r_i=g-Ax_i$</p>
<p>$r<em>{i+1} = r</em>{i}-\alpha_i Ad_i$</p>
<p>Let, we choose next residual perpendicular to previous one $r<em>{i+1}^Tr_i=r</em>{i}^Tr_{i+1}=0$.</p>
<p>$r<em>{i}^Tr</em>{i+1} = r<em>{i}^Tr</em>{i}-\alpha<em>i\, r</em>{i}^TAd_i=0$</p>
<script type="math/tex; mode=display">\alpha_i = \frac{r_{i}^Tr_{i}}{r_{i}^TAd_i}</script><p>We have to calculate $d_i$ iteratively.</p>
<p>In fact, another approach to explain this part is A-conjugate or A-orthogonal. Vector $d_i$ is n-dimensional, which is basis of A. You can find Gram Schmidt Orthogonalization if you need more information. Briefly, we can build n orthogonal basis vectors from any vector $d_0$ if A is invertible(non-zero eigen values).</p>
<script type="math/tex; mode=display">d_{i}^TAd_{i+1}=0</script><p>Let</p>
<p>$d<em>{i+1} = r</em>{i+1} + \beta<em>{i+1}d</em>{i}$</p>
<p>$d<em>{i}^TAd</em>{i+1} = d<em>{i}^TAr</em>{i+1} + \beta<em>{i+1}d</em>{i}^TAd_{i}$</p>
<p>$0 = d<em>{i}^TAr</em>{i+1} + \beta<em>{i+1}d</em>{i}^TAd_{i}$</p>
<script type="math/tex; mode=display">\beta_{i+1} = -\frac{d_{i}^TAr_{i+1}} {d_{i}^TAd_{i}}</script><p>$r<em>{i+1}^T = r</em>{i}^T-\alpha_id_i^T A\,\,\,$</p>
<p>$r<em>{i+1}^Tr</em>{i+1} = r<em>{i}^Tr</em>{i+1}-\alpha<em>id_i^T Ar</em>{i+1}\,\,\,$</p>
<p>$d<em>i^T Ar</em>{i+1}=-\frac{1}{\alpha<em>i}r</em>{i+1}^Tr_{i+1}\,\,\,$</p>
<p>$d<em>{i} = r</em>{i} + \beta<em>{i}d</em>{i-1}$</p>
<p>$Ad<em>{i} = Ar</em>{i} + \beta<em>{i}Ad</em>{i-1}$</p>
<p>$d<em>{i}^TAd</em>{i} = d<em>{i}^TAr</em>{i} + \beta<em>{i}d</em>{i}^TAd_{i-1}$</p>
<p>$d<em>{i}^TAd</em>{i} = d<em>{i}^TAr</em>{i}$</p>
<script type="math/tex; mode=display">\beta_{i+1} = -\frac{d_{i}^TAr_{i+1}}{d_{i}^TAd_{i}} = \frac{1}{\alpha_i}\frac{r_{i+1}^Tr_{i+1}}{d_{i}^TAd_{i}}= \frac{r_{i+1}^Tr_{i+1}}{r_{i}^Tr_{i}}</script><h5 id="Algorithm"><a href="#Algorithm" class="headerlink" title="Algorithm"></a>Algorithm</h5><ol>
<li>Initial guess $x_0$  </li>
<li>Set initial condition $d_0=r_0=b-Ax_0$  </li>
<li>Repeat until $r_0$ is sufficiently small(gradient of objective is near zero or linear system is near solution)<ul>
<li>compute $\alpha$<script type="math/tex; mode=display">\alpha_i = \frac{r_{i}^Tr_{i}}{r_{i}^TAd_i}\,or\,\frac{r_{i}^Tr_{i}}{d_{i}^TAd_i}</script></li>
<li>update $x$<script type="math/tex; mode=display">x_{i+1}=x_{i}+\alpha_i d_i</script></li>
<li>update $r$<script type="math/tex; mode=display">r_{i+1} = r_{i}-\alpha_i Ad_i</script></li>
<li>compute $\beta$<script type="math/tex; mode=display">\beta_{i+1} = \frac{r_{i+1}^Tr_{i+1}} {r_{i}^Tr_{i}}</script></li>
<li>update $d$<script type="math/tex; mode=display">d_{i+1} = r_{i+1} + \beta_{i+1}d_{i}</script></li>
</ul>
</li>
<li>Return $x_n$  </li>
</ol>
<h5 id="How-to-compute-Ad-i"><a href="#How-to-compute-Ad-i" class="headerlink" title="How to compute $Ad_i$"></a>How to compute $Ad_i$</h5><p>We use conjugate gradient method to avoid caculating inverse of hessian matrix. However, there is one problem left. We need A matrix in algorithm, which is KL divergence of hessian. It is well-know that caculating gradient is computationally cheap than getting hessian itself. Therfore, we use under equation.  </p>
<script type="math/tex; mode=display">\nabla^2 KL\: d_i = \nabla(\nabla KL\: d_i)</script><p><center ><br>where $\nabla^2 KL$ is $A$</p>
<p><strong>To be specific</strong></p>
<ol>
<li>Get $KL(\pi,\pi_{old})$</li>
<li>Use auto grad package to get $\nabla KL(\pi,\pi_{old})$</li>
<li>Get $\nabla KL(\pi,\pi<em>{old})d</em>{i}$</li>
<li>Again, use auto grad package to get  $\nabla(\nabla KL(\pi,\pi<em>{old})d</em>{i})$  </li>
</ol>
<p>Now, you get $Ad_i$ indirectly</p>
<h5 id="Summarry"><a href="#Summarry" class="headerlink" title="Summarry"></a>Summarry</h5><ol>
<li>Compute search direction($A^{-1}\,\nabla L$) using conjugate gradient method(n-step)<ul>
<li>Input : initial guess of $x_0$ ex. zero vector and gradient of objective function</li>
<li>In algorithm, use hessian vector prouct to get $Ad_i$ indirecty</li>
</ul>
</li>
<li>Get step size Using equation</li>
<li>Update parmeter Vector($\theta$)</li>
</ol>
<script type="math/tex; mode=display">\theta= \theta_{old}+{\sqrt{\frac{2\delta}{\nabla L^T A^{-1}\nabla L}}}A^{-1}\nabla L</script>
    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/RL/" rel="tag"># RL</a>
              <a href="/tags/Do-it/" rel="tag"># Do it</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2020/06/24/Adavantage-Weighted-Regression-Literature-Survey/" rel="prev" title="Adavantage Weighted Regression-Literature Survey">
      <i class="fa fa-chevron-left"></i> Adavantage Weighted Regression-Literature Survey
    </a></div>
      <div class="post-nav-item">
    <a href="/2020/07/02/mujoco-py-Note/" rel="next" title="Mujoco-py Note(continue)">
      Mujoco-py Note(continue) <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          
    <div class="comments" id="gitalk-container"></div>

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#Policy-Improvement-2002-rightarrow-2015"><span class="nav-number">1.</span> <span class="nav-text">Policy Improvement(2002 $\rightarrow$ 2015)</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Policy-Objective-surrogate-objective"><span class="nav-number">1.1.</span> <span class="nav-text">Policy Objective(surrogate objective)</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Boundness"><span class="nav-number">1.1.1.</span> <span class="nav-text">Boundness</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Toward-Theory-to-Practical-Implementation-form"><span class="nav-number">1.1.2.</span> <span class="nav-text">Toward Theory to Practical Implementation form</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Details-Optional"><span class="nav-number">2.</span> <span class="nav-text">Details(Optional)</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Lagragian-Method"><span class="nav-number">2.1.</span> <span class="nav-text">Lagragian Method</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Conjugate-Gradient-Method"><span class="nav-number">2.2.</span> <span class="nav-text">Conjugate Gradient Method</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Derivation-One-Minute-Derivation-of-The-Conjugate-Gradient-Algorithm"><span class="nav-number">2.2.1.</span> <span class="nav-text">Derivation(One-Minute Derivation of The Conjugate Gradient Algorithm)</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Algorithm"><span class="nav-number">2.2.2.</span> <span class="nav-text">Algorithm</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#How-to-compute-Ad-i"><span class="nav-number">2.2.3.</span> <span class="nav-text">How to compute $Ad_i$</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Summarry"><span class="nav-number">2.2.4.</span> <span class="nav-text">Summarry</span></a></li></ol></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Donghyun Sung"
      src="/images/avatar.jpg">
  <p class="site-author-name" itemprop="name">Donghyun Sung</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">6</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">11</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">4</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/DonghyunSung-MS" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;DonghyunSung-MS" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="/dh-sung@naver.com" title="E-Mail → dh-sung@naver.com"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Donghyun Sung</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

<link rel="stylesheet" href="//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.css">

<script>
NexT.utils.loadComments(document.querySelector('#gitalk-container'), () => {
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js', () => {
    var gitalk = new Gitalk({
      clientID    : 'a7e410fac468f5b553b9',
      clientSecret: '9b77cc9a4bfb24c5b1523a1774adfa2f93eb410a',
      repo        : 'donghyunsung-ms.github.io',
      owner       : 'DonghyunSung-MS',
      admin       : ['DonghyunSung-MS'],
      id          : '3fc87f2c1ddca87cde490316f92435f6',
        language: 'en',
      distractionFreeMode: true
    });
    gitalk.render('gitalk-container');
  }, window.Gitalk);
});
</script>

</body>
</html>
